# -*- coding: utf-8 -*-
"""Question_Generator_Experiment v2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zuaY48o4sEqBUeXaySyhwbmoAQ03Aj61

# Setup and Preparation

## Package Download and Installation
"""

# !pip install fuzzywuzzy
# !pip install python-Levenshtein
# !pip install unidecode
# !pip install --upgrade nltk

import sys

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

import nltk
# nltk.download('punkt')
# nltk.download('wordnet')

from gensim.models.fasttext import FastText

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""## Loads Indonesian FastText Model"""

model = FastText.load_fasttext_format('models/word-embedding/cc.id.300.bin')

EMBEDDING_SIZE = len(model.wv['a'])
print(EMBEDDING_SIZE)
print(len(model.wv.vocab))

i = 0
for key, _ in model.wv.vocab.items():
    print(key)
    i += 1
    if i > 10: break

print("algoritmatik" in model.wv.vocab)
# print(model.wv.most_similar("algoritmatik"))

"""## Loading Data"""

SQUAD_TRAIN_DATASET_PATH = 'data/processed/train-v2.0-translated_fixed_enhanced.json'
SQUAD_TEST_DATASET_PATH = 'data/processed/dev-v2.0-translated_fixed_enhanced.json'

df_squad = pd.read_json(SQUAD_TRAIN_DATASET_PATH)
df_squad_test = pd.read_json(SQUAD_TEST_DATASET_PATH)
print(df_squad.shape)
print(df_squad)
print(df_squad_test.shape)
print(df_squad_test)

# TAKEN_DATA_INDEX = 288

# df_squad = df_squad[df_squad.index <= TAKEN_DATA_INDEX]
print(df_squad.iloc[-1]['paragraphs'][-1].get('entities'))
print(df_squad.iloc[-1]['paragraphs'][-1].get('postags'))

"""## Delete Unfound Answers from Dataset"""

def delete_unfound_answers(df_squad):
    total_questions_before = 0
    total_questions = 0
    for taken_topic_idx in range(df_squad.shape[0]):
        for taken_context_idx in range(len(df_squad.iloc[taken_topic_idx]['paragraphs'])):
            i = 0
            qas = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['qas']
            while i < len(qas):
                total_questions_before += 1
                indonesian_answer = qas[i].get('indonesian_answers') or qas[i].get('indonesian_plausible_answers')
                if indonesian_answer[0]['answer_start'] < 0:
                    qas.pop(i)
                else:
                    i += 1
                    total_questions += 1
    print(f'Left: {total_questions}. Deleted: {total_questions_before-total_questions}')
print('Deleting unfound answer...')
print('Train')
delete_unfound_answers(df_squad)
print('Test')
delete_unfound_answers(df_squad_test)

"""# Prepare Tensors *(will delete tensors with unfound answers on the process)*"""

"""## TextDict Class"""

class TextDict:
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = {}
        self.index2word = {}
        self.n_words = 0

    def addWords(self, list_of_words):
        for word in list_of_words:
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

"""## Context, NER, and POS Tags TextDict"""

PAD_TOKEN = '<pad>'
EOS_TOKEN = '<eos>'
SOS_TOKEN = '<sos>'
OOV_TOKEN = '<unk>'
PAD = 0
SOS = 2
EOS = 1
OOV = 3

text_dict = TextDict('context')
text_dict.addWords([PAD_TOKEN, EOS_TOKEN, SOS_TOKEN, OOV_TOKEN])


NONE_NER_POS = '<none>'
NONE_NER_POS_TOKEN = 0

postags_textdict = TextDict('postags')
postags_textdict.addWords([NONE_NER_POS, 'NNO', 'NNP', 'PRN', 'PRR', 'PRI', 'PRK', 'ADJ', 'VBI', 'VBT', 'VBP', 'VBL', 'VBE', 'ADV', 'ADK', 'NEG', 'CCN', 'CSN', 'PPO', 'INT', 'KUA', 'NUM', 'ART', 'PAR', 'UNS', '$$$', 'SYM', 'PUN', 'TAME'])

ner_textdict = TextDict('entities')
ner_textdict.addWords([NONE_NER_POS, 'PER', 'NOR', 'FAC', 'ORG', 'GPE', 'LOC', 'PRO', 'EVT', 'WOA', 'LAW', 'LNG', 'DTE', 'TME', 'PCT', 'MON', 'QUA', 'ORD', 'CRD'])

"""## Preprocessor Functions

### Tokenizer
"""

import re
import unicodedata
from unidecode import unidecode

# Complete punctuation from string.punctuation: !"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~

def unicode_to_ascii(s):
    return unidecode(s)
    # return ''.join(
    #     c for c in unicodedata.normalize('NFD', s)
    #     if unicodedata.category(c) != 'Mn'
    # )

non_ascii_regex = re.compile(r"[^\x00-\x7F\u2013]")
def remove_non_ascii(s):
    return re.sub(non_ascii_regex, r"", s)

def normalize_string(s):
    s = unicode_to_ascii(s)
    # s = remove_non_ascii(s)
    return s

punctuations = '!"#$%&\'()*+/;<=>@?[\\]^_`{|}~'
punctuations_regex = re.compile(r"([%s])" % punctuations)
real_separator_regex = re.compile(r"([\.,:][^a-zA-Z0-9])")
def tokenize(s):
    s = re.sub(punctuations_regex, r" \1 ", s)
    s = re.sub(real_separator_regex, r" \1", s)
    s = s.split()
    return s
text = '"Frédéric Chopin! adalah, (1907–1986) (1907-1986) anak-anak (children): gembala. Andi\'s sheep is an \'03 R&B player; 奉獻 km² Jīdū °5 вера ʰp". Totalnya 10.000 rupiah'
# text = 'Beyoncé Giselle Knowles-Carter (/ biːˈjɒnseɪ / bee-YON-say) (lahir 4 September 1981) adalah penyanyi, penulis lagu, produser dan aktris rekaman Amerika. Dilahirkan dan dibesarkan di Houston, Texas, ia tampil di berbagai kompetisi menyanyi dan menari sebagai seorang anak, dan mulai terkenal pada akhir 1990-an sebagai penyanyi utama grup gadis R&B Destiny\'s Child. Dikelola oleh ayahnya, Mathew Knowles, kelompok ini menjadi salah satu kelompok gadis terlaris di dunia sepanjang masa. Hiatus mereka melihat rilis album debut Beyoncé, Dangerously in Love (2003), yang menetapkan dia sebagai artis solo di seluruh dunia, memperoleh lima Grammy Awards dan menampilkan Billboard Hot 100 nomor satu single "Crazy in Love" dan "Baby Boy" .'
tokenize(normalize_string(text))

"""### Padding"""

import math
def convert_value_to_range(value, min_value, max_value, grouping_range):
    assert value >= min_value and value <= max_value and min_value != max_value
    n_class = math.ceil((max_value-min_value+1) / grouping_range)
    val_start = min_value + (value//grouping_range) * grouping_range
    val_end = min(max_value, val_start + grouping_range - 1)
    return f'{val_start}-{val_end}'

convert_value_to_range(15, 1, 19, 5)

TAKEN_QUANTILE = 0.99 #param {type:"number"}

def pad_tensor(tensor, value, length):
    tensor.extend([value for _ in range(length - len(tensor))])

def get_sentence_and_question_max_length(taken_quantile):
    sentence_lengths = []
    for paragraph in df_squad['paragraphs']:
        for qas in paragraph:
            context_sentences = nltk.tokenize.sent_tokenize(qas['context'])
            for sentence in context_sentences:
                sentence_lengths.append(len(tokenize(normalize_string(sentence))))
    df_sentence_lengths = pd.DataFrame(sentence_lengths)
    sentence_lengths_desc = df_sentence_lengths.describe()
    print(sentence_lengths_desc, end='\n\n')

    question_lengths = []
    for paragraph in df_squad['paragraphs']:
        for qas in paragraph:
            for qa in qas['qas']:
                question_lengths.append(len(tokenize(qa['question'])))
    df_question_lengths = pd.DataFrame(question_lengths)
    question_lengths_desc = df_question_lengths.describe()
    print(question_lengths_desc, end='\n\n')

    print(df_sentence_lengths[0].apply(convert_value_to_range,
                                       args=(
                                           df_sentence_lengths[0].min(),
                                           df_sentence_lengths[0].max(), 5
                                           )
                                       ).value_counts().plot(kind='bar', title='Persebaran Panjang Kalimat Bacaan'))
    print(df_question_lengths[0].apply(convert_value_to_range,
                                       args=(
                                           df_question_lengths[0].min(),
                                           df_question_lengths[0].max(), 5
                                           )
                                       ).value_counts().plot(kind='bar', title='Persebaran Panjang Kalimat Pertanyaan'))

    sentence_quantile = df_sentence_lengths.quantile(taken_quantile)[0].astype(int)
    question_quantile = df_question_lengths.quantile(taken_quantile)[0].astype(int) + 1 # +1 for EOS token
    return sentence_quantile, question_quantile

SENTENCE_MAX_LENGTH, QUESTION_MAX_LENGTH = get_sentence_and_question_max_length(TAKEN_QUANTILE)
QUESTION_MAX_LENGTH += 1    # include EOS_TOKEN
print(SENTENCE_MAX_LENGTH)
print(QUESTION_MAX_LENGTH)

# sentence_lengths = []
# sentences = []
# for paragraph in df_squad['paragraphs']:
#     for qas in paragraph:
#         context_sentences = nltk.tokenize.sent_tokenize(qas['context'])
#         for sentence in context_sentences:
#             sentence_lengths.append(len(tokenize(normalize_string(sentence))))
#             sentences.append(sentence)
# df_sentence_lengths = pd.DataFrame(sentence_lengths)
# sentence_lengths_desc = df_sentence_lengths.describe()
# print(sentence_lengths_desc, end='\n\n')

# idx = df_sentence_lengths[df_sentence_lengths[0]==SENTENCE_MAX_LENGTH]['sentence'].index.tolist()[0]
# sentences[idx]

"""### Answer Preprocessor"""

from fuzzywuzzy import fuzz

WORD_SIMILARITY_THRESHOLD = 80

def convert_charloc_to_wordloc(tokenized_context, tokenized_words, char_loc):
    if len(tokenized_words) == 0:
        return -2

    pointer_loc = 0
    i = 0
    j = 0
    while i < len(tokenized_context) and j < min(2, len(tokenized_words)):
        if char_loc-pointer_loc <= 5:
            if tokenized_context[i].isnumeric():
                similarity = fuzz.ratio(tokenized_context[i], tokenized_words[j])
            else:
                similarity = fuzz.partial_ratio(tokenized_context[i], tokenized_words[j])
            # print(f'{tokenized_context[i]} vs {tokenized_words[j]} = {similarity}')
            if similarity >= WORD_SIMILARITY_THRESHOLD:
                j += 1
        pointer_loc += len(tokenized_context[i]) + 1
        i += 1
    if j >= min(2, len(tokenized_words)):
        return i-j
    else:
        return -1

def is_end_punctuations(token):
    return token in '.!?'

def get_sentence_location_from_answer_word_index(tokenized_context, answer_word_idx):
    start_idx = answer_word_idx-1
    end_idx = answer_word_idx
    while start_idx > -1 and not is_end_punctuations(tokenized_context[start_idx]):
        start_idx -= 1
    while end_idx < len(tokenized_context)-1 and not is_end_punctuations(tokenized_context[end_idx]):
        end_idx += 1
    return start_idx+1, end_idx

context = 'Aku adalah. Anak gembala! Selalu riang.serta gembira? Karena aku raj!in bek?erja tak pernah lengah ataupun lelah.. Lalala'
tokenized_context = tokenize(normalize_string(context))
answer_idx = 0
start_idx, end_idx = get_sentence_location_from_answer_word_index(tokenized_context, answer_idx)
print(' '.join(tokenized_context[start_idx:end_idx+1]))

# for topic in df_squad.iloc[2]['paragraphs']:
#     print(topic['context'])

# context_idx = 0
# topic_idx = 0
# qa_idx = 2

# tokenized_context = tokenize(normalize_string(df_squad.iloc[context_idx]['paragraphs'][topic_idx]['context']))
# count_tobe_removed_chars = len(re.findall(non_ascii_regex, unicodeToAscii(df_squad.iloc[context_idx]['paragraphs'][topic_idx]['context']))) * 1.5
# print(count_tobe_removed_chars)
# qa = df_squad.iloc[context_idx]['paragraphs'][topic_idx]['qas'][qa_idx]
# indonesian_answer = qa.get('indonesian_answers') or qa.get('indonesian_plausible_answers')
# answer = indonesian_answer[0]
# tokenized_words = tokenize(normalize_string(answer['text']))
# print(df_squad.iloc[context_idx]['paragraphs'][topic_idx]['context'])
# print(answer['text'])
# print(tokenized_context)
# print(tokenized_words)
# print(qa)

# answer_idx = convert_charloc_to_wordloc(tokenized_context, tokenized_words, answer['answer_start'] - count_tobe_removed_chars)

# print(tokenized_context[answer_idx:answer_idx+len(tokenized_words)])
# print(tokenized_words)


# # def convert_word_to_word_embedding(tokenized_words, model, target_shape, conversion_purpose:str, taken_topic_idx=-1, taken_context_idx=-1, qas_idx=-1):
# #     tensor = []
# #     for i in range(len(tokenized_words)):
# #       try:
# #           word_embedding = model.wv[tokenized_words[i]]
# #       except KeyError as e:
# #           print(f'{conversion_purpose} exception {e} at topic_idx={taken_topic_idx}, taken_context_idx={taken_context_idx}, qas_idx={qas_idx}')
# #           word_embedding = model.wv[OOV_REPLACEMENT]
# #       finally:
# #           tensor.append(word_embedding)
# #     tensor = np.array(tensor)
# #     padded_tensor = np.zeros(target_shape)
# #     padded_tensor[:tensor.shape[0], :tensor.shape[1]] = tensor
# #     return padded_tensor

# #            input_weights_matrix.append(np.concatenate((context_tensor, answer_tensor), axis=1))

"""### NER Preprocessor"""

def create_ner_tensor(tokenized_context, entities, ner_textdict):
    ner_tensor = [0 for _ in range(len(tokenized_context))]

    if len(entities) == 0:
        return ner_tensor

    pointer_loc = 0
    i = 0
    j = 0
    k = 0
    entities_name = tokenize(entities[j]['name'])
    while i < len(tokenized_context) and entities_name != None:
        pointer_loc += len(tokenized_context[i]) + 1
        if entities[j]['begin_offset']-pointer_loc <= 0:
            similarity = fuzz.partial_ratio(tokenized_context[i], entities_name[k])
            # print(f'{tokenized_context[i]} vs {entities_name[k]} = {similarity}')
            if similarity >= WORD_SIMILARITY_THRESHOLD:
                ner_tensor[i] = ner_textdict.word2index[entities[j]['type']]
                k += 1
                if k == len(entities_name):
                    j += 1
                    k = 0
                    entities_name = None if j == len(entities) else tokenize(entities[j]['name'])
            i += 1
    
    return ner_tensor

context = df_squad.iloc[0]['paragraphs'][0].get('context')
print(context)
entities = df_squad.iloc[0]['paragraphs'][0].get('entities')
print(entities)
print(create_ner_tensor(tokenize(normalize_string(context)), entities, ner_textdict))

"""### PosTags Preprocessor"""

FULL_MATCH = 100

def flatten(list):
    new_list = []
    for list_ in list:
        new_list.extend(list_)
    return new_list

def calc_n_gram_similarity(n, token_1, postags, j):
    n_gram = ''
    if j+n < len(postags):
        for k in range(n):
            n_gram += postags[j+k][0]
        # print(f'{n}-gram: {token_1} vs {n_gram} = {fuzz.ratio(token_1, n_gram)}')
        return fuzz.ratio(token_1, n_gram)
    else:
        return 0

MAX_N_GRAM = 5
def create_postags_tensor(tokenized_context, postags_, postags_textdict):
    pos_tensor = [0 for _ in range(len(tokenized_context))]

    if len(postags_) == 0:
        return pos_tensor

    average_sim = []
    j = 0
    postags = flatten(postags_)
    for i in range(len(tokenized_context)):
        n = 1
        found = False
        iter_limit = MAX_N_GRAM - max(0, i + 5 - len(tokenized_context))
        prev_n_gram_similarity = 0
        while n <= iter_limit and not found:
            n_gram_similarity = calc_n_gram_similarity(n, tokenized_context[i], postags, j)
            if n_gram_similarity == 0:
                pos_tensor[i] = NONE_NER_POS_TOKEN
                found = True
            elif n_gram_similarity != FULL_MATCH and n < iter_limit:
                if n_gram_similarity > prev_n_gram_similarity:
                    prev_n_gram_similarity = n_gram_similarity
                elif n_gram_similarity <= prev_n_gram_similarity:
                    j -= 1
                    pos_tensor[i] = postags_textdict.word2index[postags[j-n+1][1]]
                    # print(f'\t{tokenized_context[i]} {postags[j-n+1][1]}')
                    j += n
                    found = True
            elif n_gram_similarity >= WORD_SIMILARITY_THRESHOLD:
                pos_tensor[i] = postags_textdict.word2index[postags[j-n+1][1]]
                # print(f'\t{tokenized_context[i]} {postags[j-n+1][1]}')
                j += n
                found = True
            else:
                pos_tensor[i] = NONE_NER_POS_TOKEN
                found = True
            n += 1
        average_sim.append(n_gram_similarity if n_gram_similarity > prev_n_gram_similarity else prev_n_gram_similarity)
    # average_sim = sum(average_sim)/len(average_sim)
    # print(f'Average similarity: {average_sim:.2f}%')
    return pos_tensor

taken_topic_idx = 0
taken_content_idx = 10
context = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('context')
print(context)
postags = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('postags')
print(postags)
print(create_postags_tensor(tokenize(normalize_string(context)), postags, postags_textdict))

# results = []
# for taken_topic_idx in range(df_squad.shape[0]):
#     for taken_content_idx in range(len(df_squad.iloc[taken_topic_idx]['paragraphs'])):
#         context = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('context')
#         # print(context)
#         postags = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('postags')
#         # print(postags)
#         # print(create_postags_tensor(tokenize(normalize_string(context)), postags, postags_textdict))
#         _, result = create_postags_tensor(tokenize(normalize_string(context)), postags, postags_textdict)
#         results.append(result)
#         if result < 95:
#             print(f'{taken_topic_idx}, {taken_content_idx}, {result}')

"""## Action!"""

import string
import time 

def do_preprocess(df_squad, for_training:bool) -> (any, list, list, list):
    input_tensors = []
    is_answer_tensors = []
    is_cased_tensors = []
    ner_tensors = []
    pos_tensors = []

    target_tensors = []

    deleted = 0

    start_time = time.time()
    for taken_topic_idx in range(df_squad.shape[0]):
        for taken_context_idx in range(len(df_squad.iloc[taken_topic_idx]['paragraphs'])):
            context = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['context']
            tokenized_context = tokenize(normalize_string(context))
            if for_training: 
                text_dict.addWords(tokenized_context)
            count_tobe_removed_chars = len(re.findall(non_ascii_regex, unicode_to_ascii(context))) * 1.5  # With assumption every nonascii is followed by space

            try:
                entities = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['entities']
                postags = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['postags']
            except KeyError:
                print(f'Key Error, skipped ({taken_topic_idx}, {taken_content_idx})')
                i += 1
                continue
            entities = create_ner_tensor(tokenized_context, entities, ner_textdict)
            postags = create_postags_tensor(tokenized_context, postags, postags_textdict)

            qas = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['qas']
            i = 0
            while i < len(qas):
                qa = qas[i]

                indonesian_answer = qa.get('indonesian_answers') or qa.get('indonesian_plausible_answers')
                # indonesian_answer = qa.get('indonesian_answers')
                # if indonesian_answer is None:
                #     print(f'Found impossible question, Skipped..')
                #     i += 1
                #     continue
                tokenized_answers = tokenize(normalize_string(indonesian_answer[0]['text']))
                answer_start = indonesian_answer[0]['answer_start'] - count_tobe_removed_chars
                answer_idx = convert_charloc_to_wordloc(tokenized_context, tokenized_answers, answer_start)
                if answer_idx < 0:
                    print(f'Not found at topic_idx={taken_topic_idx}, taken_context_idx={taken_context_idx}, qas_idx={i}')
                    deleted += 1
                    qas.pop(i)
                    continue

                sent_start_idx, sent_end_idx = get_sentence_location_from_answer_word_index(tokenized_context, answer_idx)
                tokenized_sentence = tokenized_context[sent_start_idx:sent_end_idx+1]
                answer_idx -= sent_start_idx
                if for_training:
                    sentence_tensor = [text_dict.word2index[word] for word in tokenized_sentence]
                else:
                    sentence_tensor = [
                        text_dict.word2index.get(word) if text_dict.word2index.get(word) \
                        else OOV \
                        for word in tokenized_sentence
                    ]
                if sent_end_idx-sent_start_idx+1 > SENTENCE_MAX_LENGTH:
                    # print(f'Sentence too long, skipped ({taken_topic_idx}, {taken_content_idx})')
                    i += 1
                    continue

                ner_tensor = [entities[i] for i in range(sent_start_idx, sent_end_idx+1)]
                pos_tensor = [postags[i] for i in range(sent_start_idx, sent_end_idx+1)]
                pad_tensor(sentence_tensor, PAD, SENTENCE_MAX_LENGTH)
                pad_tensor(ner_tensor, NONE_NER_POS_TOKEN, SENTENCE_MAX_LENGTH)
                pad_tensor(pos_tensor, NONE_NER_POS_TOKEN, SENTENCE_MAX_LENGTH)

                is_answer_tensor = []
                is_cased_tensor = []
                for j in range(SENTENCE_MAX_LENGTH):
                    is_answer_tensor.append(
                        1 if j in range(answer_idx, answer_idx+len(tokenized_answers)) \
                        else 0
                    )
                    is_cased_tensor.append(
                        1 if j<len(tokenized_sentence) and any(c.isupper() for c in tokenized_sentence[j]) \
                        else 0
                    )

                indonesian_question = qa['question']
                tokenized_questions = tokenize(normalize_string(indonesian_question))
                if len(tokenized_questions)+1 > QUESTION_MAX_LENGTH:
                    # print(f'Question too long, skipped ({taken_topic_idx}, {taken_content_idx})')
                    i += 1
                    continue
                if for_training:
                    text_dict.addWords(tokenized_questions)
                    # if tokenized_questions[0].lower() != "kapan":
                    #     # print(f'Only accept "kapan" question. Skipping..')
                    #     i += 1
                    #     continue
                    question_tensor = [text_dict.word2index[word] for word in tokenized_questions]
                else:
                    question_tensor = [
                        text_dict.word2index.get(word) if text_dict.word2index.get(word) \
                        else OOV \
                        for word in tokenized_questions
                    ]
                question_tensor.append(EOS)
                pad_tensor(question_tensor, PAD, QUESTION_MAX_LENGTH)

                input_tensors.append(sentence_tensor)
                is_answer_tensors.append(is_answer_tensor)
                is_cased_tensors.append(is_cased_tensor)
                ner_tensors.append(ner_tensor)
                pos_tensors.append(pos_tensor)
                target_tensors.append(question_tensor)

                i += 1
    end_time = time.time()
    print(f'Not found answers: {deleted}')
    print(f'Execution time: {end_time-start_time}')
    return df_squad, input_tensors, [is_answer_tensors, is_cased_tensors, ner_tensors, pos_tensors], target_tensors

df_squad, input_tensors, feature_tensors, target_tensors = do_preprocess(df_squad, for_training=True)

df_squad_test, input_test_tensors, feature_test_tensors, target_test_tensors = do_preprocess(df_squad_test, for_training=False)

def print_input_and_answer(input_tensors, is_answer_tensors, text_dict, n=3):
    for i in range(len(input_tensors)):
        sentence_tensor = input_tensors[i]
        is_answer_tensor = is_answer_tensors[i]
        sentence = ' '.join([text_dict.index2word[word_idx] for word_idx in sentence_tensor])
        answer = ' '.join([text_dict.index2word[sentence_tensor[idx]] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
        print(sentence)
        print(f'Answer: {answer}')
        if i > n:
            break

print('Train dataset:')
print_input_and_answer(input_tensors, feature_tensors[0], text_dict)
print()
print('Test dataset :')
print_input_and_answer(input_test_tensors, feature_test_tensors[0], text_dict)

def concat_feature_tensors(*args):
    n_features = len(args)
    assert n_features > 1
    concated = np.array(args[0]).reshape(len(args[0]), -1, 1)
    for i in range(n_features-1):
        concated = np.concatenate(
            (concated, np.array(args[i+1]).reshape(len(args[i+1]), -1, 1)),
            axis = -1
            )
    return concated

def prepare_tensors(input_tensors, list_of_feature_tensors, target_tensors):
    feature_tensors = concat_feature_tensors(*list_of_feature_tensors)

    input_tensors = torch.from_numpy(np.array(input_tensors)).long().view(len(input_tensors), -1, 1).to(device)
    feature_tensors = torch.tensor(feature_tensors).float().to(device)
    target_tensors = torch.from_numpy(np.array(target_tensors)).long().view(len(target_tensors), -1, 1).to(device)
    print('Input size:', input_tensors.size())
    print('Features size:', feature_tensors.size())
    print('Target size:', target_tensors.size())
    return input_tensors, feature_tensors, target_tensors

print('Preparing train tensors')
input_tensors, feature_tensors, target_tensors = prepare_tensors(input_tensors, feature_tensors, target_tensors)

print()

print('Preparing test tensors')
input_test_tensors, feature_test_tensors, target_test_tensors = prepare_tensors(input_test_tensors, feature_test_tensors, target_test_tensors)

# Count rows which contain non-zero element
print('Train:', np.unique(feature_tensors.cpu().numpy().nonzero()[0]).shape[0])
print('Test:', np.unique(feature_test_tensors.cpu().numpy().nonzero()[0]).shape[0])

"""## Create Weights Matrix Embedding using FastText"""

weights_matrix = np.zeros((len(text_dict.index2word), EMBEDDING_SIZE))

def convert_word_to_word_embedding(word, model):
    try:
        word_embedding = model.wv[word]
    except KeyError as e:
        print(f'{type(e).__name__} exception: {e}.')
        word_embedding = model.wv[OOV_TOKEN]
    return word_embedding

for i, word in text_dict.index2word.items():
    weights_matrix[i] = convert_word_to_word_embedding(word, model)
print(f'All unknown words will be replaced with {OOV_TOKEN}')

weights_matrix = torch.from_numpy(weights_matrix)
print(f'Weights matrix size: {weights_matrix.size()}')
print(f'Total unique words count: {text_dict.n_words}')

import gc
gc.collect()

"""# Sequence to Sequence

## Helper Function
"""

import time
import math
import random
import os


os.environ['PYTHONHASHSEED'] = '42'
np.random.seed(42)
random.seed(42)
torch.manual_seed(42)


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return f'{m}m {s}s'


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return f'{asMinutes(s)} (- {asMinutes(rs)})'


def count_n_parameters(model):
    model_parameters = filter(lambda p: p.requires_grad, model.parameters())
    return sum([np.prod(p.size()) for p in model_parameters])

"""### Embedding Layer"""

def create_emb_layer(weights_matrix, non_trainable=False):
    vocab_size, embedding_dim = weights_matrix.size()
    emb_layer = nn.Embedding(vocab_size, embedding_dim)
    emb_layer.load_state_dict({'weight': weights_matrix})
    if non_trainable:
        emb_layer.weight.requires_grad = False

    return emb_layer, vocab_size, embedding_dim

"""### Plotting Results"""

plt.switch_backend('agg')
import matplotlib.ticker as ticker


def showPlot(points, save=False, path='./figures.jpg'):
    plt.figure()
    fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)
    if save:
        plt.savefig(path)
    else:
        plt.show()

"""### Evaluation"""

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
# from nltk.translate.meteor_score import exact_match

def calculate_eval_score(target, prediction):
    cc = SmoothingFunction()
    bleu_1 = sentence_bleu(target, prediction, smoothing_function=cc.method4, weights=(1, 0, 0, 0))
    bleu_2 = sentence_bleu(target, prediction, smoothing_function=cc.method4, weights=(0, 1, 0, 0))
    bleu_3 = sentence_bleu(target, prediction, smoothing_function=cc.method4, weights=(0, 0, 1, 0))
    bleu_4 = sentence_bleu(target, prediction, smoothing_function=cc.method4, weights=(0, 0, 0, 1))
    # meteor = exact_match(' '.join(target[0]), ' '.join(prediction))
    return [bleu_1, bleu_2, bleu_3, bleu_4]

def remove_eos_and_pad(word_list):
    result = []
    i = 0
    while i < len(word_list) and word_list[i] != EOS_TOKEN:
        result.append(word_list[i])
        i += 1
    return result

# """## Vanilla GRU/LSTM

# ### Model

# #### Encoder GRU/LSTM
# """

# class EncoderRNN(nn.Module):
#     def __init__(self, weights_matrix, feature_tensors, hidden_size, rnn_type='gru', bidirectional=False):
#         super(EncoderRNN, self).__init__()
#         self.rnn_type = rnn_type
#         self.bidirectional = bidirectional
#         self.hidden_size = hidden_size

#         self.embedding, _, embedding_dim = create_emb_layer(weights_matrix)
#         intermediate_dim = embedding_dim + feature_tensors.size(2)
#         if rnn_type == 'gru':
#             self.rnn = nn.GRU(intermediate_dim, hidden_size, bidirectional=bidirectional)
#         elif rnn_type == 'lstm':
#             self.rnn = nn.LSTM(intermediate_dim, hidden_size, bidirectional=bidirectional)
#         else:
#             raise RuntimeError('Encoder type must be gru or lstm!')

#     def forward(self, input, feature_tensor, hidden):
#         embedded = self.embedding(input).view(1, 1, -1)
#         output = torch.cat((embedded, feature_tensor.view(1, 1, -1)), 2)
#         output, hidden = self.rnn(output, hidden)
#         return output, hidden

#     def _createHidden(self):
#         layer = 2 if self.bidirectional else 1
#         return torch.zeros(layer, 1, self.hidden_size, device=device)

#     def initHidden(self):
#         bidir_mult = 2 if self.bidirectional else 1
#         if self.rnn_type == 'gru':
#             hidden = self._createHidden()
#         elif self.rnn_type == 'lstm':
#             hidden = (self._createHidden(), self._createHidden())
#         else:
#             raise RuntimeError('Encoder type must be gru or lstm!')
#         return hidden

# """#### Decoder GRU/LSTM (Unused)"""

# # class DecoderRNN(nn.Module):
# #     def __init__(self, weights_matrix, hidden_size, output_size):
# #         super(DecoderRNN, self).__init__()
# #         self.hidden_size = hidden_size

# #         self.embedding, _, embedding_dim = create_emb_layer(weights_matrix)
# #         self.lstm = nn.LSTM(embedding_dim, hidden_size)
# #         self.out = nn.Linear(hidden_size, output_size)
# #         self.softmax = nn.LogSoftmax(dim=1)

# #     def forward(self, input, hidden):
# #         output = self.embedding(input).view(1, 1, -1)
# #         output = F.relu(output)
# #         output, hidden = self.gru(output, hidden)
# #         output = self.softmax(self.out(output[0]))
# #         return output, hidden

# #     def initHidden(self):
# #         return torch.zeros(1, 1, self.hidden_size, device=device)

# """#### Attention Decoder GRU/LSTM"""

# class AttnDecoderRNN(nn.Module):
#     def __init__(self, weights_matrix, hidden_size, output_size, rnn_type='gru', dropout_p=0.1, max_length=SENTENCE_MAX_LENGTH):
#         super(AttnDecoderRNN, self).__init__()
#         self.rnn_type = rnn_type
#         self.hidden_size = hidden_size
#         self.output_size = output_size
#         self.dropout_p = dropout_p
#         self.max_length = max_length

#         self.embedding, _, embedding_dim = create_emb_layer(weights_matrix)
#         attn_input_dim = embedding_dim + hidden_size * (1 if rnn_type=='gru' else 2)
#         self.attn = nn.Linear(attn_input_dim, self.max_length)
#         self.attn_combine = nn.Linear(embedding_dim + hidden_size, self.hidden_size)
#         self.dropout = nn.Dropout(self.dropout_p)
#         if rnn_type == 'gru':
#             self.rnn = nn.GRU(self.hidden_size, hidden_size)
#         elif rnn_type == 'lstm':
#             self.rnn = nn.LSTM(self.hidden_size, hidden_size)
#         else:
#             raise RuntimeError('Decoder type must be gru or lstm!')
#         self.out = nn.Linear(self.hidden_size, self.output_size)

#     def forward(self, input, hidden, encoder_outputs):
#         embedded = self.embedding(input).view(1, 1, -1)
#         embedded = self.dropout(embedded)

#         if self.rnn_type == 'gru':
#             attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)
#         elif self.rnn_type == 'lstm':
#             attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0][0], hidden[1][0]), 1)), dim=1)
#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),
#                                  encoder_outputs.unsqueeze(0))

#         output = torch.cat((embedded[0], attn_applied[0]), 1)
#         output = self.attn_combine(output).unsqueeze(0)

#         output = F.relu(output)
#         output, hidden = self.rnn(output, hidden)

#         output = F.log_softmax(self.out(output[0]), dim=1)
#         return output, hidden, attn_weights

# """### Train"""

# teacher_forcing_ratio = 1


# def train(input_tensor, feature_tensor, target_tensor, encoder, decoder, criterion, max_length=SENTENCE_MAX_LENGTH):
#     encoder_hidden = encoder.initHidden()

#     input_timestep = input_tensor.size(0)
#     target_length = target_tensor.size(0)

#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

#     loss = 0

#     for ei in range(input_timestep):
#         encoder_output, encoder_hidden = encoder(
#             input_tensor[ei], feature_tensor[ei], encoder_hidden)
#         if encoder.bidirectional:
#             encoder_outputs[ei] = encoder_output[0, 0, :encoder.hidden_size] + \
#                                   encoder_output[0, 0, encoder.hidden_size:]
#         else:
#             encoder_outputs[ei] = encoder_output[0, 0]

#     decoder_input = torch.tensor([[SOS]], device=device)

#     if isinstance(encoder_hidden, tuple):
#         decoder_hidden = (encoder_hidden[0][-1:], encoder_hidden[1][-1:])
#     else:
#         decoder_hidden = encoder_hidden[-1:]

#     use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

#     if use_teacher_forcing:
#         # Teacher forcing: Feed the target as the next input
#         for di in range(target_length):
#             decoder_output, decoder_hidden, decoder_attention = decoder(
#                 decoder_input, decoder_hidden, encoder_outputs)
#             loss += criterion(decoder_output, target_tensor[di])
#             decoder_input = target_tensor[di]  # Teacher forcing
#             if decoder_input.item() == EOS:
#                 break

#     else:
#         # Without teacher forcing: use its own predictions as the next input
#         for di in range(target_length):
#             decoder_output, decoder_hidden, decoder_attention = decoder(
#                 decoder_input, decoder_hidden, encoder_outputs)
#             topv, topi = decoder_output.topk(1)
#             decoder_input = topi.squeeze().detach()  # detach from history as input

#             loss += criterion(decoder_output, target_tensor[di])
#             if decoder_input.item() == EOS:
#                 break

#     loss.backward()

#     return loss.item() / di

# SAVE_PATH = 'Checkpoints/checkpoint_gru.tar'
# N_TAKEN_DATA = input_tensors.size(0)

# def create_optimizer(optimizer, parameters, lr, betas=(0.9, 0.999)):
#     if optimizer == 'sgd':
#         return optim.SGD(parameters, lr)
#     elif optimizer == 'adam':
#         return optim.Adam(parameters, lr, betas)

# def trainIters(encoder, decoder, n_epochs, batch_size=1, checkpoint=None, print_every=1000, plot_every=100, save_every=-1, learning_rate=0.01, optimizer='adam'):
#     start = time.time()
#     plot_losses = []
#     print_loss_total = 0  # Reset every print_every
#     plot_loss_total = 0  # Reset every plot_every
#     prev_epoch = 1
#     prev_iter = 0

#     encoder_optimizer = create_optimizer(optimizer, encoder.parameters(), lr=learning_rate)
#     decoder_optimizer = create_optimizer(optimizer, decoder.parameters(), lr=learning_rate)
#     # encoder_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(encoder_optimizer, step_scheduler)
#     # decoder_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(decoder_optimizer, step_scheduler)
#     criterion = nn.NLLLoss()
#     encoder.train()
#     decoder.train()

#     if checkpoint is not None:
#         prev_epoch = checkpoint['epoch']
#         prev_iter = checkpoint['iter']
#         plot_losses = checkpoint['plot_losses']
#         encoder.load_state_dict(checkpoint['encoder_state_dict'])
#         encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])
#         encoder_optimizer.param_groups[0]['lr'] = learning_rate
#         decoder.load_state_dict(checkpoint['decoder_state_dict'])
#         decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])
#         decoder_optimizer.param_groups[0]['lr'] = learning_rate
#         print_loss_total = checkpoint['loss']
#         plot_loss_total = checkpoint['loss']
#         # encoder_lr_scheduler.load_state_dict(checkpoint['scheduler'])
#         # decoder_lr_scheduler.load_state_dict(checkpoint['scheduler'])

#     n_iters = N_TAKEN_DATA

#     prev_epoch = epoch_start = prev_epoch + (prev_iter + 1) // n_iters
#     prev_iter  = prev_iter % n_iters
#     iter_start = prev_iter + 1

#     print(f'Starting from epoch-{epoch_start}')
#     print(f'Starting from iteration-{iter_start}')
#     print(f'With LR: encoder:{encoder_optimizer.param_groups[0]["lr"]}, decoder:{decoder_optimizer.param_groups[0]["lr"]}')
#     for epoch in range(epoch_start, n_epochs + 1):
#         for iter in range(iter_start, n_iters + 1):
#             input_tensor = input_tensors[iter % n_iters]
#             feature_tensor = feature_tensors[iter % n_iters]
#             target_tensor = target_tensors[iter % n_iters]

#             if iter % batch_size == 0:
#                 encoder_optimizer.zero_grad()
#                 decoder_optimizer.zero_grad()

#             loss = train(input_tensor, feature_tensor, target_tensor, encoder,
#                         decoder, criterion)
#             print_loss_total += loss
#             plot_loss_total += loss

#             # opt.param_groups[0]['lr'] = 1e-4
#             # lr_scheduler.base_lrs=[1e-4]
#             # print('change', lr_scheduler.state_dict())

#             if iter % batch_size == batch_size-1 or iter == n_iters:
#                 encoder_optimizer.step()
#                 decoder_optimizer.step()

#             # if iter == n_iters:
#             #     encoder_lr_scheduler.step()
#             #     decoder_lr_scheduler.step()

#             if iter % print_every == 0:
#                 print_loss_avg = print_loss_total / print_every
#                 print_loss_total = 0
#                 progress_percent = ((epoch-1)*n_iters+iter - ((prev_epoch-1)*n_iters+prev_iter)) / (n_epochs*n_iters - ((prev_epoch-1)*n_iters+prev_iter))
#                 print('%s (%d-%d %.2f%%) %.4f' % \
#                       (timeSince(start, progress_percent),
#                       epoch, iter, progress_percent * 100, print_loss_avg))
#                 # print(f'LR: {encoder_lr_scheduler.get_lr()}')

#             if iter % plot_every == 0:
#                 plot_loss_avg = plot_loss_total / plot_every
#                 plot_losses.append(plot_loss_avg)
#                 plot_loss_total = 0

#             if (save_every != -1 and iter % save_every == 0) or iter==n_iters:
#                 torch.save({
#                     'epoch': epoch,
#                     'iter': iter,
#                     'plot_losses': plot_losses,
#                     'encoder_state_dict': encoder.state_dict(),
#                     'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),
#                     'decoder_state_dict': decoder.state_dict(),
#                     'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),
#                     'loss': loss
#                     # 'scheduler': encoder_lr_scheduler.state_dict()
#                 }, SAVE_PATH)
#         iter_start = 0
#         # encoder_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(encoder_optimizer, step_scheduler)
#         # decoder_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(decoder_optimizer, step_scheduler)

#     return plot_losses

# """#### Beam Search Reference
# https://github.com/budzianowski/PyTorch-Beam-Search-Decoding/blob/master/decode_beam.py

# ### Start Training
# """

# LOAD = False #@param {type:"boolean"}

# RNN_TYPE = "lstm" #@param ["gru", "lstm"]
# BIDIRECTIONAL = True #@param {type:"boolean"}
# N_EPOCHS =  1 #@param {type:"integer"}
# BATCH_SIZE = 4 #@param {type:"integer"}
# PRINT_EVERY = 10 #@param {type:"integer"}
# PLOT_EVERY = 100 #@param {type:"integer"}
# SAVE_EVERY =  1000 #@param {type:"integer"}
# LEARNING_RATE = 5e-4 #@param {type:"number"}
# OPTIMIZER = "adam" #@param ["sgd", "adam"]
# # STEP_SCHEDULER = 100 #@param {type:"integer"}
# # RESET_LR_EVERY_N_FRACTION_EPOCHS = 0.5 #@param {type:"number"}
# # LR_MULTIPLIER = 0.5 #@param {type:"number"}

# hidden_size = 256 #@param {type:"integer"}

# try:
#     checkpoint = torch.load(SAVE_PATH) if LOAD else None
# except Exception as e:
#     print(f'Unable to load checkpoint. Starting training from scratch.')
#     checkpoint = None

# encoder1 = EncoderRNN(weights_matrix, feature_tensors, hidden_size, rnn_type=RNN_TYPE, bidirectional=BIDIRECTIONAL).to(device)
# attn_decoder1 = AttnDecoderRNN(weights_matrix, hidden_size, text_dict.n_words, rnn_type=RNN_TYPE, dropout_p=0.1).to(device)

# print(f'Encoder Parameters: {count_n_parameters(encoder1)}')
# print(f'Decoder Parameters: {count_n_parameters(attn_decoder1)}')

# history = trainIters(encoder1, attn_decoder1, n_epochs=N_EPOCHS, batch_size=BATCH_SIZE, checkpoint=checkpoint, print_every=PRINT_EVERY, plot_every=PLOT_EVERY, save_every=SAVE_EVERY, learning_rate=LEARNING_RATE, optimizer=OPTIMIZER)
# showPlot(history)
# encoder1.eval()
# attn_decoder1.eval()

# plot = checkpoint['plot_losses']
# showPlot(plot)

# # encoder_optimizer = create_optimizer('adam', encoder1.parameters(), lr=1e-10)
# # encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])
# # encoder_optimizer.param_groups[0]['lr'] = 1e-10
# # encoder_optimizer.param_groups[0]['lr']

# # !cp "Checkpoints/checkpoint_gru.tar" "Checkpoints/checkpoint_gru_1epochs_116609data_2.tar"
# !cp "Checkpoints/checkpoint_gru_1epochs_116609data_2.tar" "Checkpoints/checkpoint_gru.tar"

# calculate_eval_score(['aku adalah anak gembala'.split()], 'beta adalah anak peternak'.split())

# evaluateRandomly(encoder1, attn_decoder1)

# encoder = encoder1
# decoder = attn_decoder1

# bleus = []

# start_time = time.time()
# for i in range(0, min(input_test_tensors.size(0), N_TAKEN_DATA)):
#     # input_tensor = input_tensors[i]
#     # feature_tensor = feature_tensors[i]
#     # target_tensor = target_tensors[i]
#     # is_answer_tensor = feature_tensor[:,0]
#     input_tensor = input_test_tensors[i]
#     feature_tensor = feature_test_tensors[i]
#     target_tensor = target_test_tensors[i]
#     # is_answer_tensor = feature_tensor[:,0]
    
#     input_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in input_tensor])
#     target_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensor])
#     # answer_text = ' '.join([text_dict.index2word[input_tensor[idx].item()] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
#     # print('>', input_text)
#     # print('=', target_text)
#     # print('-', answer_text)
#     output_text, attentions, eval_score = evaluate(encoder, decoder, input_tensor, feature_tensor, target_text)
#     output_sentence = ' '.join(output_text)
#     bleus.append(eval_score)
#     # print('<', output_sentence)
#     # print(f'BLEU score: {eval_score}')
#     # print()
#     if i % 500 == 0:
#         print(f'{i}: {time.time()-start_time:.2f}s')

# bleus = np.array(bleus)
# print(f'Average BLEU-1: {(np.average(bleus[:,0]))}')
# print(f'Average BLEU-2: {(np.average(bleus[:,1]))}')
# print(f'Average BLEU-3: {(np.average(bleus[:,2]))}')
# print(f'Average BLEU-4: {(np.average(bleus[:,3]))}')

# """### Evaluate"""

# def evaluate(encoder, decoder, input_tensor, feature_tensor, target_text, max_length=SENTENCE_MAX_LENGTH):
#     with torch.no_grad():
#         # input_tensor = tensorFromSentence(input_lang, sentence)
#         input_length = input_tensor.size()[0]
#         # encoder_hidden = (encoder.initHidden(), encoder.initHidden())
#         encoder_hidden = encoder.initHidden()

#         encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

#         for ei in range(input_length):
#             encoder_output, encoder_hidden = encoder(input_tensor[ei],
#                                                      feature_tensor[ei],
#                                                      encoder_hidden)
#             encoder_outputs[ei] += encoder_output[0, 0]

#         decoder_input = torch.tensor([[SOS]], device=device)  # SOS

#         decoder_hidden = encoder_hidden

#         decoded_words = []
#         decoder_attentions = torch.zeros(max_length, max_length)

#         for di in range(max_length):
#             decoder_output, decoder_hidden, decoder_attention = decoder(
#                 decoder_input, decoder_hidden, encoder_outputs)
#             decoder_attentions[di] = decoder_attention.data
#             topv, topi = decoder_output.data.topk(1)
#             if topi.item() == EOS:
#                 decoded_words.append('<eos>')
#                 break
#             else:
#                 decoded_words.append(text_dict.index2word[topi.item()])

#             decoder_input = topi.squeeze().detach()
        
#         eval_score = calculate_eval_score(
#             [remove_eos_and_pad(target_text.split())],
#             remove_eos_and_pad(decoded_words)
#         )

#         return decoded_words, decoder_attentions[:di + 1], eval_score

# def evaluateRandomly(encoder, decoder, n=10):
#     for _ in range(n):
#         i = random.randint(0, input_tensors.size()[0])
#         input_tensor = input_tensors[i]
#         feature_tensor = feature_tensors[i]
#         target_tensor = target_tensors[i]
#         is_answer_tensor = feature_tensor[:,0]
#         input_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in input_tensor])
#         target_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensor])
#         answer_text = ' '.join([text_dict.index2word[input_tensor[idx].item()] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
#         print('>', input_text)
#         print('=', target_text)
#         print('-', answer_text)
#         output_text, attentions, eval_score = evaluate(encoder, decoder, input_tensor, feature_tensor, target_text)
#         output_sentence = ' '.join(output_text)
#         print('<', output_sentence)
#         print(f'Bleu score: {eval_score}')
#         print('')

# output_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensors[0]])
# print(output_text)

# """#### Visualize Attention"""

# chosen = 3000
# input_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in input_tensors[chosen]])
# output_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensors[chosen]])
# print(input_text)
# print(output_text)

# output_words, attentions = evaluate(
#     encoder1, attn_decoder1, input_tensors[chosen], feature_tensors[chosen])
# plt.matshow(attentions.numpy())

# """For a better viewing experience we will do the extra work of adding axes
# and labels:
# """

# def showAttention(input_sentence, output_words, attentions):
#     # Set up figure with colorbar
#     fig = plt.figure()
#     ax = fig.add_subplot(111)
#     cax = ax.matshow(attentions.numpy(), cmap='bone')
#     fig.colorbar(cax)

#     # Set up axes
#     ax.set_xticklabels([''] + input_sentence.split(' ') +
#                        ['<EOS>'], rotation=90)
#     ax.set_yticklabels([''] + output_words)

#     # Show label at every tick
#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

#     plt.show()


# def evaluateAndShowAttention(input_sentence):
#     output_words, attentions = evaluate(
#         encoder1, attn_decoder1, input_sentence)
#     print('input =', input_sentence)
#     print('output =', ' '.join(output_words))
#     showAttention(input_sentence, output_words, attentions)


# evaluateAndShowAttention("elle a cinq ans de moins que moi .")

# evaluateAndShowAttention("elle est trop petit .")

# evaluateAndShowAttention("je ne crains pas de mourir .")

# evaluateAndShowAttention("c est un jeune directeur plein de talent .")

"""## Transformer

### Model
"""

from torch.nn import Transformer, Linear
import math

class TransformerModel(nn.Module):
    def __init__(self, weights_matrix, feature_tensors, nhead, nlayers, nhid, dropout=0.5):
        super(TransformerModel, self).__init__()
        self.model_type = 'Transformer'
        self.feature_count = 0 if feature_tensors is None else feature_tensors.size(-1)
        self.src_mask = None
        self.tgt_mask = None

        self.embedding, vocab_size, embedding_dim = create_emb_layer(weights_matrix)
        self.input_dim = embedding_dim + self.feature_count

        self.pos_encoding = PositionalEncoding(self.input_dim, dropout)
        self.transformer = Transformer(self.input_dim, nhead, nlayers, nlayers, nhid, dropout)
        self.linear = nn.Linear(self.input_dim, vocab_size)

        self.init_weights()

    def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def init_weights(self):
        initrange = 0.1
        self.linear.bias.data.zero_()
        self.linear.weight.data.uniform_(-initrange, initrange)

    def prepare_mask(self, data):
        device = data.device
        mask = self._generate_square_subsequent_mask(len(data)).to(device)
        return mask

    def prepare_input(self, data, features=None):
        device = data.device
        if features is None or features.size(0) != self.feature_count:
            features = torch.zeros(()).new_full((data.size(0), data.size(1), self.feature_count), 0).to(device)
        # print('initial:', data.size())
        # print(features.size())
        data = self.embedding(data).view(data.size(0), data.size(1), -1)
        # print('embedded:', data.size())
        data = torch.cat((data, features), 2).view(data.size(1), data.size(0), -1)
        # print('final:', data.size())
        data = data * math.sqrt(self.input_dim)
        return self.pos_encoding(data)

    def forward(self, src, features, tgt):
        # if self.src_mask is None or self.src_mask.size(0) != len(src):
        #     device = src.device
        #     mask = self._generate_square_subsequent_mask(len(src)).to(device)
        #     self.src_mask = mask
        if self.tgt_mask is None or self.tgt_mask.size(0) != len(tgt):
            device = tgt.device
            mask = self._generate_square_subsequent_mask(len(tgt)).to(device)
            self.tgt_mask = mask

        src = self.prepare_input(src, features)
        tgt = self.prepare_input(tgt)

        output = self.transformer(src, tgt, src_mask=None, tgt_mask=self.tgt_mask)
        # print('output:', output.size())
        output = self.linear(output)
        # print('output linearized:', output.size())
        return output

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

"""### Train"""

def get_batch(train_data, feature_data, target_data, batch_size, i):
    i_end = min(i+batch_size, train_data.size(0))
    return train_data[i:i_end], feature_data[i:i_end], target_data[i:i_end]

def train_transformer(model, criterion, optimizer, scheduler, train_data, feature_data, target_data, batch_size, curr_epoch, start_batch=0, plot_losses=[], log_interval=100, plot_interval=1000, save_interval=10000):
    model.train() # Turn on the train mode
    print_total_loss = 0.
    plot_total_loss = 0.
    start_time = time.time()
    vocab_size = weights_matrix.size(0)
    n_batch = math.ceil(len(train_data) / batch_size)
    for batch, i in enumerate(range(start_batch*batch_size, train_data.size(0), batch_size), start_batch):
        # data, features, targets = get_batch(train_data, feature_data, target_data, batch_size, i)
        i_end = min(i+batch_size, train_data.size(0))
        optimizer.zero_grad()
        for j in range(i, i_end):
            targets = target_data[j].unsqueeze(0)
            output = model(train_data[j].unsqueeze(0), feature_data[j].unsqueeze(0), targets)
            # print(output.view(1, vocab_size, -1).size())
            # print(targets.squeeze(-1).size())
            loss = criterion(
                output.view(1, vocab_size, -1), 
                targets.squeeze(-1)
                )
            loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()

        print_total_loss += loss.item()
        plot_total_loss += loss.item()
        if batch % log_interval == 0 and batch > 0:
            cur_loss = print_total_loss / log_interval
            elapsed = time.time() - start_time
            print('| epoch {:3d} | {:5d}/{:5d} batches | '
                  'lr {:02.2f} | ms/batch {:5.2f} | '
                  'loss {:5.2f} | ppl {:8.2f}'.format(
                    curr_epoch, batch, n_batch, scheduler.get_lr()[0],
                    elapsed * 1000 / log_interval,
                    cur_loss, math.exp(cur_loss)))
            print_total_loss = 0
            start_time = time.time()

        if (batch % plot_interval == 0 and batch > 0) or batch==n_batch-1:
            plot_losses_avg = plot_total_loss / (batch % plot_interval or plot_interval)
            plot_losses.append(plot_losses_avg)
            plot_total_loss = 0

        if batch==n_batch-1:
            scheduler.step()

        if (batch % save_interval == 0 and batch > 0) or batch==n_batch-1:
            next_batch = (batch+1) % n_batch
            next_epoch = curr_epoch + (batch+1) // n_batch
            torch.save({
                'next_epoch': next_epoch,
                'next_batch': next_batch,
                'plot_losses': plot_losses,
                'state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler': scheduler.state_dict()
            }, SAVE_PATH_TRANSFORMER)
    return plot_losses

def validate_transformer(eval_model, data_source):
    eval_model.eval() # Turn on the evaluation mode
    total_loss = 0.
    vocab_size = weights_matrix.size(0)
    with torch.no_grad():
        for i in range(0, data_source.size(0) - 1, batch_size):
            data, targets = get_batch(data_source, i)
            output = eval_model(data)
            output_flat = output.view(-1, vocab_size)
            total_loss += len(data) * criterion(output_flat, targets).item()
    return total_loss / (len(data_source) - 1)

"""Loop over epochs. Save the model if the validation loss is the best
we've seen so far. Adjust the learning rate after each epoch.
"""

def trainIters_transformer(model, input_tensors, feature_tensors, target_tensors, epochs, learning_rate=5.0, checkpoint=None):
    # best_val_loss = float("inf")
    # best_model = None
    start_epoch = 1
    start_batch = 0
    plot_losses = []

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)

    if checkpoint is not None:
        start_epoch = checkpoint['next_epoch']
        start_batch = checkpoint['next_batch']
        plot_losses = checkpoint['plot_losses']
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        # optimizer.param_groups[0]['lr'] = learning_rate
        scheduler.load_state_dict(checkpoint['scheduler'])

    print(f'Starting from epoch-{start_epoch}')
    print(f'Starting from batch-{start_batch}')
    for epoch in range(start_epoch, epochs + 1):
        epoch_start_time = time.time()
        losses = train_transformer(
            model, criterion, optimizer, scheduler, \
            input_tensors, feature_tensors, target_tensors, \
            batch_size=4, curr_epoch=epoch, start_batch=start_batch,
            plot_losses=plot_losses
            )
        plot_losses.extend(losses)
        # val_loss = evaluate_transformer(model, val_data)
        avg_loss = -1 if losses==[] else sum(losses)/len(losses)
        start_batch = 0
        print('-' * 89)
        # print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
        #       'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time)))
        #                                 ,val_loss, math.exp(val_loss)))
        print('| end of epoch {:3d} | time: {:5.2f}s | avg loss {:5.2f} | avg ppl {:8.2f}' \
              .format(epoch, (time.time() - epoch_start_time), avg_loss, math.exp(avg_loss)))
        print('-' * 89)

        # if val_loss < best_val_loss:
        #     best_val_loss = val_loss
        #     best_model = model
    return plot_losses

"""### Start Training"""

SAVE_PATH_TRANSFORMER = 'models/checkpoints/checkpoint_transformer.tar'

LOAD = True
nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder
nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder
nhead = 2 # the number of heads in the multiheadattention models
dropout = 0.2 # the dropout value

n_epoch = 20
learning_rate = 5.0

try:
    checkpoint = torch.load(SAVE_PATH_TRANSFORMER) if LOAD else None
except Exception as e:
    print(f'Unable to load checkpoint. Starting training from scratch.')
    checkpoint = None

transformer_model = TransformerModel(weights_matrix, feature_tensors, nhead, nlayers, nhid, dropout).to(device)
print(f'Transformer Parameters: {count_n_parameters(transformer_model)}')
history = trainIters_transformer(transformer_model, input_tensors, feature_tensors, target_tensors, n_epoch, learning_rate, checkpoint)
showPlot(history, save=True)

# plot = checkpoint['plot_losses']
# showPlot(plot)

"""### Evaluate"""

def evaluate_transformer(model, input_tensor, feature_tensor, target_tensor):
    with torch.no_grad():
        target_length = target_tensor.size(0)

        input_tensor = input_tensor.unsqueeze(0)
        feature_tensor = feature_tensor.unsqueeze(0)
        target_tensor = target_tensor.unsqueeze(0)

        # print(input_tensor.size())
        # print(feature_tensor.size())
        # print(target_tensor.size())
        
        src_mask = model.prepare_mask(input_tensor)
        src = model.prepare_input(input_tensor, feature_tensor)
        memory = model.transformer.encoder(src, mask=src_mask)

        output = [text_dict.word2index[SOS_TOKEN]]
        # pad_tensor(output, PAD, QUESTION_MAX_LENGTH)
        tgt = torch.tensor(output).view(1, 1, 1).to(device)
        for i in range(target_length):
            tgt_mask = model.prepare_mask(target_tensor)
            tgt = model.prepare_input(tgt)
            out = model.transformer.decoder(tgt, memory, tgt_mask=tgt_mask)
            out = model.linear(out)
            out = F.softmax(out, dim=-1)
            # print(out.size())
            topv, topi = out.data.topk(2)
            # print(topi.size())
            prediction = topi[i,0][0].item()
            output.append(prediction)
            # print(output)
            if prediction == EOS:
                break
            tgt = torch.tensor(output).view(1, -1, 1).to(device)
        decoded_words = [text_dict.index2word[idx] for idx in output[1:]]

        target_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensor.squeeze(0)])
        eval_score = calculate_eval_score(
            [remove_eos_and_pad(target_text.split())],
            remove_eos_and_pad(decoded_words)
        )

        return decoded_words, eval_score

def evaluate_transformer_randomly(model, input_tensors, feature_tensors, target_tensors, n=10):
    for _ in range(n):
        i = random.randint(0, input_tensors.size()[0])
        input_tensor = input_tensors[i]
        feature_tensor = feature_tensors[i]
        target_tensor = target_tensors[i]
        is_answer_tensor = feature_tensor[:,0]
        input_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in input_tensor])
        target_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensor])
        answer_text = ' '.join([text_dict.index2word[input_tensor[idx].item()] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
        print('>', input_text)
        print('=', target_text)
        print('-', answer_text)
        output_text, eval_score = evaluate_transformer(model, input_tensor, feature_tensor, target_tensor)
        output_sentence = ' '.join(output_text)
        print('<', output_sentence)
        print(f'Bleu score: {eval_score}')
        print('')

evaluate_transformer_randomly(transformer_model, input_tensors, feature_tensors, target_tensors)
