# -*- coding: utf-8 -*-
"""Question_Generator_Experiment v2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zuaY48o4sEqBUeXaySyhwbmoAQ03Aj61

# Setup and Preparation

## Package Download and Installation
"""

# !pip install fuzzywuzzy
# !pip install python-Levenshtein
# !pip install unidecode
# !pip install --upgrade nltk

import sys

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

import nltk
# nltk.download('punkt')
# nltk.download('wordnet')

from gensim.models.fasttext import FastText

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BASE_PATH = './'
# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.plot([1,2,3])
plt.show()

# from google.colab import drive
# drive.mount('/content/drive')

"""## Loads Indonesian FastText Model"""

model = FastText.load_fasttext_format(f'{BASE_PATH}models/word-embedding/cc.id.300.bin')

EMBEDDING_SIZE = len(model.wv['a'])
print(EMBEDDING_SIZE)
print(len(model.wv.vocab))

i = 0
for key, _ in model.wv.vocab.items():
    print(key)
    i += 1
    if i > 10: break

print("algoritmatik" in model.wv.vocab)
# print(model.wv.most_similar("algoritmatik"))

"""## Loading Data"""

SQUAD_TRAIN_DATASET_PATH = f'{BASE_PATH}data/processed/train-v2.0-translated_fixed_enhanced.json'
SQUAD_TEST_DATASET_PATH = f'{BASE_PATH}data/processed/dev-v2.0-translated_fixed_enhanced.json'

df_squad = pd.read_json(SQUAD_TRAIN_DATASET_PATH)
df_squad_test = pd.read_json(SQUAD_TEST_DATASET_PATH)
print(df_squad.shape)
print(df_squad)
print(df_squad_test.shape)
print(df_squad_test)

# TAKEN_DATA_INDEX = 288

# df_squad = df_squad[df_squad.index <= TAKEN_DATA_INDEX]
print(df_squad.iloc[-1]['paragraphs'][-1].get('entities'))
print(df_squad.iloc[-1]['paragraphs'][-1].get('postags'))

"""## Delete Unfound Answers from Dataset"""

def delete_unfound_answers(df_squad):
    total_questions_before = 0
    total_questions = 0
    for taken_topic_idx in range(df_squad.shape[0]):
        for taken_context_idx in range(len(df_squad.iloc[taken_topic_idx]['paragraphs'])):
            i = 0
            qas = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['qas']
            while i < len(qas):
                total_questions_before += 1
                indonesian_answer = qas[i].get('indonesian_answers') or qas[i].get('indonesian_plausible_answers')
                if indonesian_answer[0]['answer_start'] < 0:
                    qas.pop(i)
                else:
                    i += 1
                    total_questions += 1
    print(f'Left: {total_questions}. Deleted: {total_questions_before-total_questions}')
print('Deleting unfound answer...')
print('Train')
delete_unfound_answers(df_squad)
print('Test')
delete_unfound_answers(df_squad_test)

"""# Prepare Tensors *(will delete tensors with unfound answers on the process)*"""

# print(model.wv.most_similar('adalah'))
# print(model.wv['∈'])
# print(model.wv.similarity('buddha', 'pisau'))
# print(model.wv.similarity('buddha', 'tripitaka'))

# print('frederic' in model.wv.vocab)
# print(model.wv.most_similar('frederic'))
# print('frédéric' in model.wv.vocab)
# print(model.wv.most_similar('frédéric'))

print(model.wv.most_similar('<unk'))
print(model.wv.most_similar('<sos>'))
print(model.wv.most_similar('<eos>'))
print(model.wv.most_similar('<pad>'))
print(model.wv.most_similar('</sos>'))
print(model.wv.most_similar('</eos>'))

"""## TextDict Class"""

class TextDict:
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = {}
        self.index2word = {}
        self.n_words = 0

    def addWords(self, list_of_words):
        for word in list_of_words:
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

"""## Context, NER, and POS Tags TextDict"""

PAD_TOKEN = '<pad>'
EOS_TOKEN = '<eos>'
SOS_TOKEN = '<sos>'
OOV_TOKEN = '<unk>'
PAD = 0
SOS = 2
EOS = 1
OOV = 3

text_dict = TextDict('context')
text_dict.addWords([PAD_TOKEN, EOS_TOKEN, SOS_TOKEN, OOV_TOKEN])


NONE_NER_POS = '<none>'
NONE_NER_POS_TOKEN = 0

postags_textdict = TextDict('postags')
postags_textdict.addWords([NONE_NER_POS, 'NNO', 'NNP', 'PRN', 'PRR', 'PRI', 'PRK', 'ADJ', 'VBI', 'VBT', 'VBP', 'VBL', 'VBE', 'ADV', 'ADK', 'NEG', 'CCN', 'CSN', 'PPO', 'INT', 'KUA', 'NUM', 'ART', 'PAR', 'UNS', '$$$', 'SYM', 'PUN', 'TAME'])

ner_textdict = TextDict('entities')
ner_textdict.addWords([NONE_NER_POS, 'PER', 'NOR', 'FAC', 'ORG', 'GPE', 'LOC', 'PRO', 'EVT', 'WOA', 'LAW', 'LNG', 'DTE', 'TME', 'PCT', 'MON', 'QUA', 'ORD', 'CRD'])

"""## Preprocessor Functions

### Tokenizer
"""

import re
import unicodedata
from unidecode import unidecode

# Complete punctuation from string.punctuation: !"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~

def unicode_to_ascii(s):
    return unidecode(s)
    # return ''.join(
    #     c for c in unicodedata.normalize('NFD', s)
    #     if unicodedata.category(c) != 'Mn'
    # )

non_ascii_regex = re.compile(r"[^\x00-\x7F\u2013]")
def remove_non_ascii(s):
    return re.sub(non_ascii_regex, r"", s)

def normalize_string(s):
    s = unicode_to_ascii(s)
    # s = remove_non_ascii(s)
    return s

punctuations = '!"#$%&\'()*+/;<=>@?[\\]^_`{|}~'
punctuations_regex = re.compile(r"([%s])" % punctuations)
real_separator_regex = re.compile(r"([\.,:][^a-zA-Z0-9])")
def tokenize(s):
    s = re.sub(punctuations_regex, r" \1 ", s)
    s = re.sub(real_separator_regex, r" \1", s)
    s = s.split()
    return s
text = '"Frédéric Chopin! adalah, (1907–1986) (1907-1986) anak-anak (children): gembala. Andi\'s sheep is an \'03 R&B player; 奉獻 km² Jīdū °5 вера ʰp". Totalnya 10.000 rupiah'
# text = 'Beyoncé Giselle Knowles-Carter (/ biːˈjɒnseɪ / bee-YON-say) (lahir 4 September 1981) adalah penyanyi, penulis lagu, produser dan aktris rekaman Amerika. Dilahirkan dan dibesarkan di Houston, Texas, ia tampil di berbagai kompetisi menyanyi dan menari sebagai seorang anak, dan mulai terkenal pada akhir 1990-an sebagai penyanyi utama grup gadis R&B Destiny\'s Child. Dikelola oleh ayahnya, Mathew Knowles, kelompok ini menjadi salah satu kelompok gadis terlaris di dunia sepanjang masa. Hiatus mereka melihat rilis album debut Beyoncé, Dangerously in Love (2003), yang menetapkan dia sebagai artis solo di seluruh dunia, memperoleh lima Grammy Awards dan menampilkan Billboard Hot 100 nomor satu single "Crazy in Love" dan "Baby Boy" .'
tokenize(normalize_string(text))

"""### Padding"""

import math
def convert_value_to_range(value, min_value, max_value, grouping_range):
    assert value >= min_value and value <= max_value and min_value != max_value
    n_class = math.ceil((max_value-min_value+1) / grouping_range)
    val_start = min_value + (value//grouping_range) * grouping_range
    val_end = min(max_value, val_start + grouping_range - 1)
    return f'{val_start}-{val_end}'

convert_value_to_range(15, 1, 19, 5)

TAKEN_QUANTILE = 0.99 #param {type:"number"}

def pad_tensor(tensor, value, length):
    tensor.extend([value for _ in range(length - len(tensor))])

def get_sentence_and_question_max_length(taken_quantile):
    sentence_lengths = []
    for paragraph in df_squad['paragraphs']:
        for qas in paragraph:
            context_sentences = nltk.tokenize.sent_tokenize(qas['context'])
            for sentence in context_sentences:
                sentence_lengths.append(len(tokenize(normalize_string(sentence))))
    df_sentence_lengths = pd.DataFrame(sentence_lengths)
    sentence_lengths_desc = df_sentence_lengths.describe()
    print(sentence_lengths_desc, end='\n\n')

    question_lengths = []
    for paragraph in df_squad['paragraphs']:
        for qas in paragraph:
            for qa in qas['qas']:
                question_lengths.append(len(tokenize(qa['question'])))
    df_question_lengths = pd.DataFrame(question_lengths)
    question_lengths_desc = df_question_lengths.describe()
    print(question_lengths_desc, end='\n\n')

    print(df_sentence_lengths[0].apply(convert_value_to_range,
                                       args=(
                                           df_sentence_lengths[0].min(),
                                           df_sentence_lengths[0].max(), 5
                                           )
                                       ).value_counts().plot(kind='bar', title='Persebaran Panjang Kalimat Bacaan'))
    print(df_question_lengths[0].apply(convert_value_to_range,
                                       args=(
                                           df_question_lengths[0].min(),
                                           df_question_lengths[0].max(), 5
                                           )
                                       ).value_counts().plot(kind='bar', title='Persebaran Panjang Kalimat Pertanyaan'))

    sentence_quantile = df_sentence_lengths.quantile(taken_quantile)[0].astype(int)
    question_quantile = df_question_lengths.quantile(taken_quantile)[0].astype(int) + 1 # +1 for EOS token
    return sentence_quantile, question_quantile

SENTENCE_MAX_LENGTH, QUESTION_MAX_LENGTH = get_sentence_and_question_max_length(TAKEN_QUANTILE)
QUESTION_MAX_LENGTH += 1    # include EOS_TOKEN
print(SENTENCE_MAX_LENGTH)
print(QUESTION_MAX_LENGTH)

# sentence_lengths = []
# sentences = []
# for paragraph in df_squad['paragraphs']:
#     for qas in paragraph:
#         context_sentences = nltk.tokenize.sent_tokenize(qas['context'])
#         for sentence in context_sentences:
#             sentence_lengths.append(len(tokenize(normalize_string(sentence))))
#             sentences.append(sentence)
# df_sentence_lengths = pd.DataFrame(sentence_lengths)
# sentence_lengths_desc = df_sentence_lengths.describe()
# print(sentence_lengths_desc, end='\n\n')

# idx = df_sentence_lengths[df_sentence_lengths[0]==SENTENCE_MAX_LENGTH]['sentence'].index.tolist()[0]
# sentences[idx]

"""### Answer Preprocessor"""

from fuzzywuzzy import fuzz

WORD_SIMILARITY_THRESHOLD = 80

def convert_charloc_to_wordloc(tokenized_context, tokenized_words, char_loc):
    if len(tokenized_words) == 0:
        return -2

    pointer_loc = 0
    i = 0
    j = 0
    while i < len(tokenized_context) and j < min(2, len(tokenized_words)):
        if char_loc-pointer_loc <= 5:
            if tokenized_context[i].isnumeric():
                similarity = fuzz.ratio(tokenized_context[i], tokenized_words[j])
            else:
                similarity = fuzz.partial_ratio(tokenized_context[i], tokenized_words[j])
            # print(f'{tokenized_context[i]} vs {tokenized_words[j]} = {similarity}')
            if similarity >= WORD_SIMILARITY_THRESHOLD:
                j += 1
        pointer_loc += len(tokenized_context[i]) + 1
        i += 1
    if j >= min(2, len(tokenized_words)):
        return i-j
    else:
        return -1

def is_end_punctuations(token):
    return token in '.!?'

def get_sentence_location_from_answer_word_index(tokenized_context, answer_word_idx):
    start_idx = answer_word_idx-1
    end_idx = answer_word_idx
    while start_idx > -1 and not is_end_punctuations(tokenized_context[start_idx]):
        start_idx -= 1
    while end_idx < len(tokenized_context)-1 and not is_end_punctuations(tokenized_context[end_idx]):
        end_idx += 1
    return start_idx+1, end_idx

context = 'Aku adalah. Anak gembala! Selalu riang.serta gembira? Karena aku raj!in bek?erja tak pernah lengah ataupun lelah.. Lalala'
tokenized_context = tokenize(normalize_string(context))
answer_idx = 0
start_idx, end_idx = get_sentence_location_from_answer_word_index(tokenized_context, answer_idx)
print(' '.join(tokenized_context[start_idx:end_idx+1]))

# for topic in df_squad.iloc[2]['paragraphs']:
#     print(topic['context'])

# context_idx = 0
# topic_idx = 0
# qa_idx = 2

# tokenized_context = tokenize(normalize_string(df_squad.iloc[context_idx]['paragraphs'][topic_idx]['context']))
# count_tobe_removed_chars = len(re.findall(non_ascii_regex, unicodeToAscii(df_squad.iloc[context_idx]['paragraphs'][topic_idx]['context']))) * 1.5
# print(count_tobe_removed_chars)
# qa = df_squad.iloc[context_idx]['paragraphs'][topic_idx]['qas'][qa_idx]
# indonesian_answer = qa.get('indonesian_answers') or qa.get('indonesian_plausible_answers')
# answer = indonesian_answer[0]
# tokenized_words = tokenize(normalize_string(answer['text']))
# print(df_squad.iloc[context_idx]['paragraphs'][topic_idx]['context'])
# print(answer['text'])
# print(tokenized_context)
# print(tokenized_words)
# print(qa)

# answer_idx = convert_charloc_to_wordloc(tokenized_context, tokenized_words, answer['answer_start'] - count_tobe_removed_chars)

# print(tokenized_context[answer_idx:answer_idx+len(tokenized_words)])
# print(tokenized_words)


# # def convert_word_to_word_embedding(tokenized_words, model, target_shape, conversion_purpose:str, taken_topic_idx=-1, taken_context_idx=-1, qas_idx=-1):
# #     tensor = []
# #     for i in range(len(tokenized_words)):
# #       try:
# #           word_embedding = model.wv[tokenized_words[i]]
# #       except KeyError as e:
# #           print(f'{conversion_purpose} exception {e} at topic_idx={taken_topic_idx}, taken_context_idx={taken_context_idx}, qas_idx={qas_idx}')
# #           word_embedding = model.wv[OOV_REPLACEMENT]
# #       finally:
# #           tensor.append(word_embedding)
# #     tensor = np.array(tensor)
# #     padded_tensor = np.zeros(target_shape)
# #     padded_tensor[:tensor.shape[0], :tensor.shape[1]] = tensor
# #     return padded_tensor

# #            input_weights_matrix.append(np.concatenate((context_tensor, answer_tensor), axis=1))

"""### NER Preprocessor"""

def create_ner_tensor(tokenized_context, entities, ner_textdict):
    ner_tensor = [0 for _ in range(len(tokenized_context))]

    if len(entities) == 0:
        return ner_tensor

    pointer_loc = 0
    i = 0
    j = 0
    k = 0
    entities_name = tokenize(entities[j]['name'])
    while i < len(tokenized_context) and entities_name != None:
        pointer_loc += len(tokenized_context[i]) + 1
        if entities[j]['begin_offset']-pointer_loc <= 0:
            similarity = fuzz.partial_ratio(tokenized_context[i], entities_name[k])
            # print(f'{tokenized_context[i]} vs {entities_name[k]} = {similarity}')
            if similarity >= WORD_SIMILARITY_THRESHOLD:
                ner_tensor[i] = ner_textdict.word2index[entities[j]['type']]
                k += 1
                if k == len(entities_name):
                    j += 1
                    k = 0
                    entities_name = None if j == len(entities) else tokenize(entities[j]['name'])
            i += 1
    
    return ner_tensor

context = df_squad.iloc[0]['paragraphs'][0].get('context')
print(context)
entities = df_squad.iloc[0]['paragraphs'][0].get('entities')
print(entities)
print(create_ner_tensor(tokenize(normalize_string(context)), entities, ner_textdict))

"""### PosTags Preprocessor"""

FULL_MATCH = 100

def flatten(list):
    new_list = []
    for list_ in list:
        new_list.extend(list_)
    return new_list

def calc_n_gram_similarity(n, token_1, postags, j):
    n_gram = ''
    if j+n < len(postags):
        for k in range(n):
            n_gram += postags[j+k][0]
        # print(f'{n}-gram: {token_1} vs {n_gram} = {fuzz.ratio(token_1, n_gram)}')
        return fuzz.ratio(token_1, n_gram)
    else:
        return 0

MAX_N_GRAM = 5
def create_postags_tensor(tokenized_context, postags_, postags_textdict):
    pos_tensor = [0 for _ in range(len(tokenized_context))]

    if len(postags_) == 0:
        return pos_tensor

    average_sim = []
    j = 0
    postags = flatten(postags_)
    for i in range(len(tokenized_context)):
        n = 1
        found = False
        iter_limit = MAX_N_GRAM - max(0, i + 5 - len(tokenized_context))
        prev_n_gram_similarity = 0
        while n <= iter_limit and not found:
            n_gram_similarity = calc_n_gram_similarity(n, tokenized_context[i], postags, j)
            if n_gram_similarity == 0:
                pos_tensor[i] = NONE_NER_POS_TOKEN
                found = True
            elif n_gram_similarity != FULL_MATCH and n < iter_limit:
                if n_gram_similarity > prev_n_gram_similarity:
                    prev_n_gram_similarity = n_gram_similarity
                elif n_gram_similarity <= prev_n_gram_similarity:
                    j -= 1
                    pos_tensor[i] = postags_textdict.word2index[postags[j-n+1][1]]
                    # print(f'\t{tokenized_context[i]} {postags[j-n+1][1]}')
                    j += n
                    found = True
            elif n_gram_similarity >= WORD_SIMILARITY_THRESHOLD:
                pos_tensor[i] = postags_textdict.word2index[postags[j-n+1][1]]
                # print(f'\t{tokenized_context[i]} {postags[j-n+1][1]}')
                j += n
                found = True
            else:
                pos_tensor[i] = NONE_NER_POS_TOKEN
                found = True
            n += 1
        average_sim.append(n_gram_similarity if n_gram_similarity > prev_n_gram_similarity else prev_n_gram_similarity)
    # average_sim = sum(average_sim)/len(average_sim)
    # print(f'Average similarity: {average_sim:.2f}%')
    return pos_tensor

taken_topic_idx = 0
taken_content_idx = 10
context = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('context')
print(context)
postags = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('postags')
print(postags)
print(create_postags_tensor(tokenize(normalize_string(context)), postags, postags_textdict))

# results = []
# for taken_topic_idx in range(df_squad.shape[0]):
#     for taken_content_idx in range(len(df_squad.iloc[taken_topic_idx]['paragraphs'])):
#         context = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('context')
#         # print(context)
#         postags = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('postags')
#         # print(postags)
#         # print(create_postags_tensor(tokenize(normalize_string(context)), postags, postags_textdict))
#         _, result = create_postags_tensor(tokenize(normalize_string(context)), postags, postags_textdict)
#         results.append(result)
#         if result < 95:
#             print(f'{taken_topic_idx}, {taken_content_idx}, {result}')

"""## Action!"""

import string
import time 

def do_preprocess(df_squad, for_training:bool) -> (any, list, list, list):
    input_tensors = []
    is_answer_tensors = []
    is_cased_tensors = []
    ner_tensors = []
    pos_tensors = []

    target_tensors = []

    deleted = 0

    start_time = time.time()
    for taken_topic_idx in range(df_squad.shape[0]):
        for taken_context_idx in range(len(df_squad.iloc[taken_topic_idx]['paragraphs'])):
            context = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['context']
            tokenized_context = tokenize(normalize_string(context))
            if for_training: 
                text_dict.addWords(tokenized_context)
            count_tobe_removed_chars = len(re.findall(non_ascii_regex, unicode_to_ascii(context))) * 1.5  # With assumption every nonascii is followed by space

            try:
                entities = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['entities']
                postags = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['postags']
            except KeyError:
                print(f'Key Error, skipped ({taken_topic_idx}, {taken_content_idx})')
                i += 1
                continue
            entities = create_ner_tensor(tokenized_context, entities, ner_textdict)
            postags = create_postags_tensor(tokenized_context, postags, postags_textdict)

            qas = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['qas']
            i = 0
            while i < len(qas):
                qa = qas[i]

                indonesian_answer = qa.get('indonesian_answers') or qa.get('indonesian_plausible_answers')
                # indonesian_answer = qa.get('indonesian_answers')
                # if indonesian_answer is None:
                #     print(f'Found impossible question, Skipped..')
                #     i += 1
                #     continue
                tokenized_answers = tokenize(normalize_string(indonesian_answer[0]['text']))
                answer_start = indonesian_answer[0]['answer_start'] - count_tobe_removed_chars
                answer_idx = convert_charloc_to_wordloc(tokenized_context, tokenized_answers, answer_start)
                if answer_idx < 0:
                    print(f'Not found at topic_idx={taken_topic_idx}, taken_context_idx={taken_context_idx}, qas_idx={i}')
                    deleted += 1
                    qas.pop(i)
                    continue

                sent_start_idx, sent_end_idx = get_sentence_location_from_answer_word_index(tokenized_context, answer_idx)
                tokenized_sentence = tokenized_context[sent_start_idx:sent_end_idx+1]
                answer_idx -= sent_start_idx
                if for_training:
                    sentence_tensor = [text_dict.word2index[word] for word in tokenized_sentence]
                else:
                    sentence_tensor = [
                        text_dict.word2index.get(word) if text_dict.word2index.get(word) \
                        else OOV \
                        for word in tokenized_sentence
                    ]
                if sent_end_idx-sent_start_idx+1 > SENTENCE_MAX_LENGTH:
                    # print(f'Sentence too long, skipped ({taken_topic_idx}, {taken_content_idx})')
                    i += 1
                    continue

                ner_tensor = [entities[i] for i in range(sent_start_idx, sent_end_idx+1)]
                pos_tensor = [postags[i] for i in range(sent_start_idx, sent_end_idx+1)]
                pad_tensor(sentence_tensor, PAD, SENTENCE_MAX_LENGTH)
                pad_tensor(ner_tensor, NONE_NER_POS_TOKEN, SENTENCE_MAX_LENGTH)
                pad_tensor(pos_tensor, NONE_NER_POS_TOKEN, SENTENCE_MAX_LENGTH)

                is_answer_tensor = []
                is_cased_tensor = []
                for j in range(SENTENCE_MAX_LENGTH):
                    is_answer_tensor.append(
                        1 if j in range(answer_idx, answer_idx+len(tokenized_answers)) \
                        else 0
                    )
                    is_cased_tensor.append(
                        1 if j<len(tokenized_sentence) and any(c.isupper() for c in tokenized_sentence[j]) \
                        else 0
                    )

                indonesian_question = qa['question']
                tokenized_questions = tokenize(normalize_string(indonesian_question))
                if len(tokenized_questions)+1 > QUESTION_MAX_LENGTH:
                    # print(f'Question too long, skipped ({taken_topic_idx}, {taken_content_idx})')
                    i += 1
                    continue
                if for_training:
                    text_dict.addWords(tokenized_questions)
                    # if tokenized_questions[0].lower() != "kapan":
                    #     # print(f'Only accept "kapan" question. Skipping..')
                    #     i += 1
                    #     continue
                    question_tensor = [text_dict.word2index[word] for word in tokenized_questions]
                else:
                    question_tensor = [
                        text_dict.word2index.get(word) if text_dict.word2index.get(word) \
                        else OOV \
                        for word in tokenized_questions
                    ]
                question_tensor.append(EOS)
                pad_tensor(question_tensor, PAD, QUESTION_MAX_LENGTH)

                input_tensors.append(sentence_tensor)
                is_answer_tensors.append(is_answer_tensor)
                is_cased_tensors.append(is_cased_tensor)
                ner_tensors.append(ner_tensor)
                pos_tensors.append(pos_tensor)
                target_tensors.append(question_tensor)

                i += 1
    end_time = time.time()
    print(f'Not found answers: {deleted}')
    print(f'Execution time: {end_time-start_time}')
    return df_squad, input_tensors, [is_answer_tensors, is_cased_tensors, ner_tensors, pos_tensors], target_tensors

df_squad, input_tensors, feature_tensors, target_tensors = do_preprocess(df_squad, for_training=True)

df_squad_test, input_test_tensors, feature_test_tensors, target_test_tensors = do_preprocess(df_squad_test, for_training=False)

def print_input_and_answer(input_tensors, is_answer_tensors, text_dict, n=3):
    for i in range(len(input_tensors)):
        sentence_tensor = input_tensors[i]
        is_answer_tensor = is_answer_tensors[i]
        sentence = ' '.join([text_dict.index2word[word_idx] for word_idx in sentence_tensor])
        answer = ' '.join([text_dict.index2word[sentence_tensor[idx]] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
        print(sentence)
        print(f'Answer: {answer}')
        if i > n:
            break

print('Train dataset:')
print_input_and_answer(input_tensors, feature_tensors[0], text_dict)
print()
print('Test dataset :')
print_input_and_answer(input_test_tensors, feature_test_tensors[0], text_dict)

def concat_feature_tensors(*args):
    n_features = len(args)
    assert n_features > 1
    concated = np.array(args[0]).reshape(len(args[0]), -1, 1)
    for i in range(n_features-1):
        concated = np.concatenate(
            (concated, np.array(args[i+1]).reshape(len(args[i+1]), -1, 1)),
            axis = -1
            )
    return concated

def prepare_tensors(input_tensors, list_of_feature_tensors, target_tensors):
    feature_tensors = concat_feature_tensors(*list_of_feature_tensors)

    input_tensors = torch.from_numpy(np.array(input_tensors)).long().view(len(input_tensors), -1, 1).to(device)
    feature_tensors = torch.tensor(feature_tensors).float().to(device)
    target_tensors = torch.from_numpy(np.array(target_tensors)).long().view(len(target_tensors), -1, 1).to(device)
    print('Input size:', input_tensors.size())
    print('Features size:', feature_tensors.size())
    print('Target size:', target_tensors.size())
    return input_tensors, feature_tensors, target_tensors

print('Preparing train tensors')
input_tensors, feature_tensors, target_tensors = prepare_tensors(input_tensors, feature_tensors, target_tensors)

print()

print('Preparing test tensors')
input_test_tensors, feature_test_tensors, target_test_tensors = prepare_tensors(input_test_tensors, feature_test_tensors, target_test_tensors)

# Count rows which contain non-zero element
print('Train:', np.unique(feature_tensors.cpu().numpy().nonzero()[0]).shape[0])
print('Test:', np.unique(feature_test_tensors.cpu().numpy().nonzero()[0]).shape[0])

"""## Create Weights Matrix Embedding using FastText"""

weights_matrix = np.zeros((len(text_dict.index2word), EMBEDDING_SIZE))

def convert_word_to_word_embedding(word, model):
    try:
        word_embedding = model.wv[word]
    except KeyError as e:
        print(f'{type(e).__name__} exception: {e}.')
        word_embedding = model.wv[OOV_TOKEN]
    return word_embedding

for i, word in text_dict.index2word.items():
    weights_matrix[i] = convert_word_to_word_embedding(word, model)
print(f'All unknown words will be replaced with {OOV_TOKEN}')

weights_matrix = torch.from_numpy(weights_matrix)
print(f'Weights matrix size: {weights_matrix.size()}')
print(f'Total unique words count: {text_dict.n_words}')

import gc
gc.collect()

"""# Sequence to Sequence

## Helper Function
"""

import time
import math
import random
import os


os.environ['PYTHONHASHSEED'] = '42'
np.random.seed(42)
random.seed(42)
torch.manual_seed(42)


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return f'{m:d}m {s:2.3f}s'


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return f'{asMinutes(s)} (- {asMinutes(rs)})'


def count_n_parameters(model):
    model_parameters = filter(lambda p: p.requires_grad, model.parameters())
    return sum([np.prod(p.size()) for p in model_parameters])

"""### Embedding Layer"""

def create_emb_layer(weights_matrix, non_trainable=False):
    vocab_size, embedding_dim = weights_matrix.size()
    emb_layer = nn.Embedding(vocab_size, embedding_dim)
    emb_layer.load_state_dict({'weight': weights_matrix})
    if non_trainable:
        emb_layer.weight.requires_grad = False

    return emb_layer, vocab_size, embedding_dim

"""### Plotting Results"""

plt.switch_backend('agg')
import matplotlib.ticker as ticker


def showPlot(points, save=False, path='./figures.jpg'):
    plt.figure()
    fig, ax = plt.subplots()
    if max(points) > 1:
        base = 0.5 if max(points) > 5 else 0.2
        loc = ticker.MultipleLocator(base=base)
        ax.yaxis.set_major_locator(loc)
    plt.plot(points)
    if save:
        plt.savefig(path)
    else:
        plt.show()

"""### Evaluation"""

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
# from nltk.translate.meteor_score import exact_match

def calculate_eval_score(target, prediction):
    cc = SmoothingFunction()
    bleu_1 = sentence_bleu(target, prediction, smoothing_function=cc.method4, weights=(1, 0, 0, 0))
    bleu_2 = sentence_bleu(target, prediction, smoothing_function=cc.method4, weights=(0, 1, 0, 0))
    bleu_3 = sentence_bleu(target, prediction, smoothing_function=cc.method4, weights=(0, 0, 1, 0))
    bleu_4 = sentence_bleu(target, prediction, smoothing_function=cc.method4, weights=(0, 0, 0, 1))
    # meteor = exact_match(' '.join(target[0]), ' '.join(prediction))
    return [bleu_1, bleu_2, bleu_3, bleu_4]

def remove_eos_and_pad(word_list):
    result = []
    i = 0
    while i < len(word_list) and word_list[i] != EOS_TOKEN:
        result.append(word_list[i])
        i += 1
    return result

"""## Vanilla GRU/LSTM

### Model

#### Encoder GRU/LSTM
"""

class EncoderRNN(nn.Module):
    def __init__(self, weights_matrix, feature_tensors, hidden_size, rnn_type='gru', bidirectional=False):
        super(EncoderRNN, self).__init__()
        self.rnn_type = rnn_type
        self.bidirectional = bidirectional
        self.hidden_size = hidden_size

        self.embedding, _, embedding_dim = create_emb_layer(weights_matrix)
        intermediate_dim = embedding_dim + feature_tensors.size(2)
        if rnn_type == 'gru':
            self.rnn = nn.GRU(intermediate_dim, hidden_size, bidirectional=bidirectional)
        elif rnn_type == 'lstm':
            self.rnn = nn.LSTM(intermediate_dim, hidden_size, bidirectional=bidirectional)
        else:
            raise RuntimeError('Encoder type must be gru or lstm!')

    def forward(self, input, feature_tensor, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output = torch.cat((embedded, feature_tensor.view(1, 1, -1)), 2)
        output, hidden = self.rnn(output, hidden)
        return output, hidden

    def _createHidden(self):
        layer = 2 if self.bidirectional else 1
        return torch.zeros(layer, 1, self.hidden_size, device=device)

    def initHidden(self):
        bidir_mult = 2 if self.bidirectional else 1
        if self.rnn_type == 'gru':
            hidden = self._createHidden()
        elif self.rnn_type == 'lstm':
            hidden = (self._createHidden(), self._createHidden())
        else:
            raise RuntimeError('Encoder type must be gru or lstm!')
        return hidden

"""#### Decoder GRU/LSTM (Unused)"""

# class DecoderRNN(nn.Module):
#     def __init__(self, weights_matrix, hidden_size, output_size):
#         super(DecoderRNN, self).__init__()
#         self.hidden_size = hidden_size

#         self.embedding, _, embedding_dim = create_emb_layer(weights_matrix)
#         self.lstm = nn.LSTM(embedding_dim, hidden_size)
#         self.out = nn.Linear(hidden_size, output_size)
#         self.softmax = nn.LogSoftmax(dim=1)

#     def forward(self, input, hidden):
#         output = self.embedding(input).view(1, 1, -1)
#         output = F.relu(output)
#         output, hidden = self.gru(output, hidden)
#         output = self.softmax(self.out(output[0]))
#         return output, hidden

#     def initHidden(self):
#         return torch.zeros(1, 1, self.hidden_size, device=device)

"""#### Attention Decoder GRU/LSTM"""

class AttnDecoderRNN(nn.Module):
    def __init__(self, weights_matrix, hidden_size, output_size, rnn_type='gru', dropout_p=0.1, max_length=SENTENCE_MAX_LENGTH):
        super(AttnDecoderRNN, self).__init__()
        self.rnn_type = rnn_type
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding, _, embedding_dim = create_emb_layer(weights_matrix)
        attn_input_dim = embedding_dim + hidden_size * (1 if rnn_type=='gru' else 2)
        self.attn = nn.Linear(attn_input_dim, self.max_length)
        self.attn_combine = nn.Linear(embedding_dim + hidden_size, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        if rnn_type == 'gru':
            self.rnn = nn.GRU(self.hidden_size, hidden_size)
        elif rnn_type == 'lstm':
            self.rnn = nn.LSTM(self.hidden_size, hidden_size)
        else:
            raise RuntimeError('Decoder type must be gru or lstm!')
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        embedded = self.dropout(embedded)

        if self.rnn_type == 'gru':
            attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)
        elif self.rnn_type == 'lstm':
            attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0][0], hidden[1][0]), 1)), dim=1)
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)

        output = F.relu(output)
        output, hidden = self.rnn(output, hidden)

        output = F.log_softmax(self.out(output[0]), dim=1)
        return output, hidden, attn_weights

"""### Train"""

teacher_forcing_ratio = 1


def train(encoder, decoder, input_tensor, feature_tensor, target_tensor, criterion):
    encoder_hidden = encoder.initHidden()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)

    loss = 0

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(
            input_tensor[ei], feature_tensor[ei], encoder_hidden)
        if encoder.bidirectional:
            encoder_outputs[ei] = encoder_output[0, 0, :encoder.hidden_size] + \
                                  encoder_output[0, 0, encoder.hidden_size:]
        else:
            encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS]], device=device)

    if isinstance(encoder_hidden, tuple):
        decoder_hidden = (encoder_hidden[0][-1:], encoder_hidden[1][-1:])
    else:
        decoder_hidden = encoder_hidden[-1:]

    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # Teacher forcing
            if decoder_input.item() == EOS:
                break

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS:
                break

    loss.backward()

    return loss.item() / di

SAVE_PATH = f'{BASE_PATH}models/checkpoints/checkpoint_lstm_bidirectional.tar'

N_PRINT_IN_EPOCH = 1000
N_PLOT_IN_EPOCH = 4
N_SAVE_IN_EPOCH = 10

def create_optimizer(optimizer, parameters, lr, betas=(0.9, 0.999)):
    if optimizer == 'sgd':
        return optim.SGD(parameters, lr)
    elif optimizer == 'adam':
        return optim.Adam(parameters, lr, betas)

def trainIters(encoder, decoder, input_tensors, feature_tensors, target_tensors, n_epochs, batch_size=1, learning_rate=0.01, optimizer='adam', scheduler_step_size=10, scheduler_gamma=0.8, checkpoint=None):
    start = time.time()
    print_every = max(1, input_tensors.size(0) // N_PRINT_IN_EPOCH)
    plot_every = max(1, input_tensors.size(0) // N_PLOT_IN_EPOCH)
    save_every = max(1, input_tensors.size(0) // N_SAVE_IN_EPOCH)

    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every
    prev_epoch = 1
    prev_iter = 0

    encoder_optimizer = create_optimizer(optimizer, encoder.parameters(), lr=learning_rate)
    decoder_optimizer = create_optimizer(optimizer, decoder.parameters(), lr=learning_rate)
    encoder_lr_scheduler = optim.lr_scheduler.StepLR(encoder_optimizer, scheduler_step_size, scheduler_gamma)
    decoder_lr_scheduler = optim.lr_scheduler.StepLR(decoder_optimizer, scheduler_step_size, scheduler_gamma)
    criterion = nn.NLLLoss()
    encoder.train()
    decoder.train()

    if checkpoint is not None:
        prev_epoch = checkpoint['epoch']
        prev_iter = checkpoint['iter']
        plot_losses = checkpoint['plot_losses']
        encoder.load_state_dict(checkpoint['encoder_state_dict'])
        encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])
        # encoder_optimizer.param_groups[0]['lr'] = learning_rate
        decoder.load_state_dict(checkpoint['decoder_state_dict'])
        decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])
        # decoder_optimizer.param_groups[0]['lr'] = learning_rate
        print_loss_total = checkpoint['loss']
        plot_loss_total = checkpoint['loss']
        encoder_lr_scheduler.load_state_dict(checkpoint['scheduler'])
        decoder_lr_scheduler.load_state_dict(checkpoint['scheduler'])

    n_iters = len(input_tensors)

    prev_epoch = epoch_start = prev_epoch + (prev_iter + 1) // n_iters
    prev_iter  = prev_iter % n_iters
    iter_start = prev_iter + 1

    print(f'Starting from epoch-{epoch_start}')
    print(f'Starting from iteration-{iter_start}')
    print(f'With LR: encoder:{encoder_lr_scheduler.get_lr()[0]}, decoder:{decoder_lr_scheduler.get_lr()[0]}')
    for epoch in range(epoch_start, n_epochs + 1):
        for iter in range(iter_start, n_iters + 1):
            last_plot_loss_idx = max(1, len(plot_losses))
            input_tensor = input_tensors[(iter-1) % n_iters]
            feature_tensor = feature_tensors[(iter-1) % n_iters]
            target_tensor = target_tensors[(iter-1) % n_iters]

            if iter % batch_size == 0:
                encoder_optimizer.zero_grad()
                decoder_optimizer.zero_grad()

            loss = train(encoder, decoder, input_tensor, feature_tensor, target_tensor, criterion)
            print_loss_total += loss
            plot_loss_total += loss

            if iter % batch_size == batch_size-1 or iter == n_iters:
                encoder_optimizer.step()
                decoder_optimizer.step()

            if iter == n_iters:
                encoder_lr_scheduler.step()
                decoder_lr_scheduler.step()

            if iter % print_every == 0:
                print_loss_avg = print_loss_total / (iter % print_every or print_every)
                print_loss_total = 0
                progress_percent = ((epoch-1)*n_iters+iter - ((prev_epoch-1)*n_iters+prev_iter)) / (n_epochs*n_iters - ((prev_epoch-1)*n_iters+prev_iter))
                print('%s | %d-%d %.2f%% | Loss %.4f | LR %f' % \
                      (timeSince(start, progress_percent),
                      epoch, iter, progress_percent * 100, print_loss_avg,
                       encoder_lr_scheduler.get_lr()[0]))

            if iter % plot_every == 0:
                plot_loss_avg = plot_loss_total / (iter % plot_every or plot_every)
                # print('meow:', iter, plot_loss_total, (iter % plot_every or plot_every))
                plot_losses.append(plot_loss_avg)
                plot_loss_total = 0

            if iter==n_iters:
                last_epoch_losses = plot_losses[last_plot_loss_idx-1:len(plot_losses)]
                avg_loss = sum(last_epoch_losses)/max(1, len(last_epoch_losses))
                print(f'END OF EPOCH {epoch} | Avg loss {avg_loss:.4f}')

            if (save_every != -1 and iter % save_every == 0) or iter==n_iters:
                torch.save({
                    'epoch': epoch,
                    'iter': iter,
                    'plot_losses': plot_losses,
                    'encoder_state_dict': encoder.state_dict(),
                    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),
                    'decoder_state_dict': decoder.state_dict(),
                    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),
                    'loss': loss,
                    'scheduler': encoder_lr_scheduler.state_dict()
                }, SAVE_PATH)
        iter_start = 1

    encoder.eval()
    decoder.eval()
    return plot_losses

"""#### Beam Search Reference
https://github.com/budzianowski/PyTorch-Beam-Search-Decoding/blob/master/decode_beam.py

### Start Training
"""

LOAD = True #@param {type:"boolean"}
# N_TAKEN_DATA = input_tensors.size(0)

RNN_TYPE = "lstm" #@param ["gru", "lstm"]
BIDIRECTIONAL = True #@param {type:"boolean"}
N_EPOCHS = 50 #@param {type:"integer"}
BATCH_SIZE = 4 #@param {type:"integer"}
LEARNING_RATE =  1e-3 #@param {type:"number"}
OPTIMIZER = "adam" #@param ["sgd", "adam"]
SCHEDULER_STEP_SIZE = 10 #@param {type:"integer"}
SCHEDULER_GAMMA = 0.95 #@param {type:"number"}

hidden_size = 256 #@param {type:"integer"}

try:
    checkpoint = torch.load(SAVE_PATH) if LOAD else None
except Exception as e:
    print(f'Unable to load checkpoint. Starting training from scratch.')
    checkpoint = None

encoder1 = EncoderRNN(weights_matrix, feature_tensors, hidden_size, rnn_type=RNN_TYPE, bidirectional=BIDIRECTIONAL).to(device)
attn_decoder1 = AttnDecoderRNN(weights_matrix, hidden_size, text_dict.n_words, rnn_type=RNN_TYPE, dropout_p=0.1).to(device)

print(f'Encoder Parameters: {count_n_parameters(encoder1)}')
print(f'Decoder Parameters: {count_n_parameters(attn_decoder1)}')

history = trainIters(encoder1, attn_decoder1, input_tensors, feature_tensors, target_tensors, n_epochs=N_EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE, optimizer=OPTIMIZER, scheduler_step_size=SCHEDULER_STEP_SIZE, scheduler_gamma=SCHEDULER_GAMMA, checkpoint=checkpoint)
# history = trainIters(encoder1, attn_decoder1, input_tensors[:8], feature_tensors[:8], target_tensors[:8], n_epochs=N_EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE, optimizer=OPTIMIZER, scheduler_step_size=SCHEDULER_STEP_SIZE, scheduler_gamma=SCHEDULER_GAMMA, checkpoint=checkpoint)
showPlot(history, save=True)

# plot = checkpoint['plot_losses']
# showPlot(plot)

# encoder_optimizer = create_optimizer('adam', encoder1.parameters(), lr=1e-10)
# encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])
# encoder_optimizer.param_groups[0]['lr'] = 1e-10
# encoder_optimizer.param_groups[0]['lr']

# !cp "/content/drive/My Drive/TA/checkpoints/checkpoint_gru.tar" "/content/drive/My Drive/TA/checkpoints/checkpoint_gru_1epochs_116609data_2.tar"
# !cp "/content/drive/My Drive/TA/checkpoints/checkpoint_gru_1epochs_116609data_2.tar" "/content/drive/My Drive/TA/checkpoints/checkpoint_gru.tar"

calculate_eval_score(['aku adalah anak gembala'.split()], 'beta adalah anak peternak'.split())

"""### Evaluate"""

def evaluate(encoder, decoder, input_tensor, feature_tensor, target_text, max_length=SENTENCE_MAX_LENGTH):
    with torch.no_grad():
        # input_tensor = tensorFromSentence(input_lang, sentence)
        input_length = input_tensor.size()[0]
        # encoder_hidden = (encoder.initHidden(), encoder.initHidden())
        encoder_hidden = encoder.initHidden()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden = encoder(input_tensor[ei],
                                                     feature_tensor[ei],
                                                     encoder_hidden)
            if encoder.bidirectional:
                encoder_outputs[ei] += encoder_output[0, 0, :encoder.hidden_size] + \
                                      encoder_output[0, 0, encoder.hidden_size:]
            else:
                encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS]], device=device)  # SOS

        if isinstance(encoder_hidden, tuple):
            decoder_hidden = (encoder_hidden[0][-1:], encoder_hidden[1][-1:])
        else:
            decoder_hidden = encoder_hidden[-1:]

        decoded_words = []
        decoder_attentions = torch.zeros(max_length, max_length)

        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            decoder_attentions[di] = decoder_attention.data
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == EOS:
                decoded_words.append('<eos>')
                break
            else:
                decoded_words.append(text_dict.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()
        
        eval_score = calculate_eval_score(
            [remove_eos_and_pad(target_text.split())],
            remove_eos_and_pad(decoded_words)
        )

        return decoded_words, decoder_attentions[:di + 1], eval_score

def do_evaluation(encoder, decoder, n=10, randomized=True):
    for j in range(n):
        i = j if not randomized else random.randint(0, input_tensors.size()[0])
        input_tensor = input_tensors[i]
        feature_tensor = feature_tensors[i]
        target_tensor = target_tensors[i]
        is_answer_tensor = feature_tensor[:,0]
        input_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in input_tensor])
        target_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensor])
        answer_text = ' '.join([text_dict.index2word[input_tensor[idx].item()] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
        print('>', input_text)
        print('=', target_text)
        print('-', answer_text)
        output_text, attentions, eval_score = evaluate(encoder, decoder, input_tensor, feature_tensor, target_text)
        output_sentence = ' '.join(output_text)
        print('<', output_sentence)
        print(f'Bleu score: {eval_score}')
        print('')

output_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensors[0]])
print(output_text)

do_evaluation(encoder1, attn_decoder1, 8, False)

encoder = encoder1
decoder = attn_decoder1

bleus = []

start_time = time.time()
for i in range(input_test_tensors.size(0)):
    # input_tensor = input_tensors[i]
    # feature_tensor = feature_tensors[i]
    # target_tensor = target_tensors[i]
    # is_answer_tensor = feature_tensor[:,0]
    input_tensor = input_test_tensors[i]
    feature_tensor = feature_test_tensors[i]
    target_tensor = target_test_tensors[i]
    # is_answer_tensor = feature_tensor[:,0]
    
    input_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in input_tensor])
    target_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensor])
    # answer_text = ' '.join([text_dict.index2word[input_tensor[idx].item()] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
    # print('>', input_text)
    # print('=', target_text)
    # print('-', answer_text)
    output_text, attentions, eval_score = evaluate(encoder, decoder, input_tensor, feature_tensor, target_text)
    output_sentence = ' '.join(output_text)
    bleus.append(eval_score)
    # print('<', output_sentence)
    # print(f'BLEU score: {eval_score}')
    # print()
    if i % 500 == 0:
        print(f'{i}: {time.time()-start_time:.2f}s')

bleus = np.array(bleus)
print(f'Average BLEU-1: {(np.average(bleus[:,0]))}')
print(f'Average BLEU-2: {(np.average(bleus[:,1]))}')
print(f'Average BLEU-3: {(np.average(bleus[:,2]))}')
print(f'Average BLEU-4: {(np.average(bleus[:,3]))}')
