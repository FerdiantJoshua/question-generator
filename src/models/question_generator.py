# -*- coding: utf-8 -*-
"""Question_Generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zuaY48o4sEqBUeXaySyhwbmoAQ03Aj61

# Package Download and Installation
"""

import sys

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

import nltk
nltk.download('punkt')

from gensim.models.fasttext import FastText

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

!pip install fuzzywuzzy
!pip install python-Levenshtein
!pip install unidecode

"""# Indonesian FastText Downloading and Loading

## Install FastText, and Download Datasets
"""

# !git clone https://github.com/facebookresearch/fastText.git
# import os
# os.chdir('fastText')
# !pip install .
# os.chdir('..')

# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz
# !gzip -d "cc.id.300.bin.gz"
# !cp "cc.id.300.bin" "/content/drive/My Drive/TA/FastTextIndonesia/cc.id.300.bin"

# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.id.zip
# !unzip wiki.id.zip
# !cp wiki.id.bin "/content/drive/My Drive/TA/FastTextIndonesia/wiki.id.bin"
# !cp wiki.id.vec "/content/drive/My Drive/TA/FastTextIndonesia/wiki.id.vec"


"""## Loads FastText Model"""

model = FastText.load_fasttext_format('../../models/word-embedding/FastTextIndonesia/cc.id.300.bin')

EMBEDDING_SIZE = len(model.wv['a'])
print(EMBEDDING_SIZE)
print(len(model.wv.vocab))

# print("algoritmatik" in model.wv.vocab)
# # print(model.wv.most_similar("algoritmatik"))

"""# Loading Data"""

# SQUAD_DATASET_PATH = '/content/drive/My Drive/TA/data/processed/train-v2.0-translated_fixed.json'
SQUAD_DATASET_PATH = '/../..//data/processed/train-v2.0-translated_fixed_enhanced.json'

df_squad = pd.read_json(SQUAD_DATASET_PATH)
# df_squad = df_squad.drop(columns=['failure_percentage', 'total_questions'])
print(df_squad.shape)
print(df_squad)

print(df_squad.iloc[-1]['paragraphs'][-1].get('entities'))
print(df_squad.iloc[-1]['paragraphs'][-1].get('postags'))

"""# Delete Unfound Answers from Dataset"""

total_questions_before = 0
total_questions = 0
for taken_topic_idx in range(df_squad.shape[0]):
    for taken_context_idx in range(len(df_squad.iloc[taken_topic_idx]['paragraphs'])):
        i = 0
        qas = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['qas']
        while i < len(qas):
            total_questions_before += 1
            indonesian_answer = qas[i].get('indonesian_answers') or qas[i].get('indonesian_plausible_answers')
            if indonesian_answer[0]['answer_start'] < 0:
                qas.pop(i)
            else:
                i += 1
                total_questions += 1
print(f'Left: {total_questions}. Deleted: {total_questions_before-total_questions}')

"""# Prepare Tensors and Delete Unfound Answers on the Process"""

"""## TextDict Class"""

class TextDict:
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = {}
        self.index2word = {}
        self.n_words = 0

    def addWords(self, list_of_words):
        for word in list_of_words:
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

"""## Context, NER, and POS Tags TextDict"""

PAD = '<pad>'
EOS = '<eos>'
SOS = '<sos>'
PADDING_TOKEN = 0
SOS_TOKEN = 2
EOS_TOKEN = 1

text_dict = TextDict('context')
text_dict.addWords([PAD, EOS, SOS])


NONE_NER_POS = '<none>'
NONE_NER_POS_TOKEN = 0

postags_textdict = TextDict('postags')
postags_textdict.addWords([NONE_NER_POS, 'NNO', 'NNP', 'PRN', 'PRR', 'PRI', 'PRK', 'ADJ', 'VBI', 'VBT', 'VBP', 'VBL', 'VBE', 'ADV', 'ADK', 'NEG', 'CCN', 'CSN', 'PPO', 'INT', 'KUA', 'NUM', 'ART', 'PAR', 'UNS', '$$$', 'SYM', 'PUN', 'TAME'])

ner_textdict = TextDict('entities')
ner_textdict.addWords([NONE_NER_POS, 'PER', 'NOR', 'FAC', 'ORG', 'GPE', 'LOC', 'PRO', 'EVT', 'WOA', 'LAW', 'LNG', 'DTE', 'TME', 'PCT', 'MON', 'QUA', 'ORD', 'CRD'])

"""## Preprocessor Functions

### Tokenizer
"""

import re
import unicodedata
from unidecode import unidecode

# Complete punctuation from string.punctuation: !"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~

def unicode_to_ascii(s):
    return unidecode(s)

non_ascii_regex = re.compile(r"[^\x00-\x7F\u2013]")
def remove_non_ascii(s):
    return re.sub(non_ascii_regex, r"", s)

def normalize_string(s):
    s = unicode_to_ascii(s)
    # s = remove_non_ascii(s)
    return s

punctuations = '!"#$%&\'()*+/;<=>@?[\\]^_`{|}~'
punctuations_regex = re.compile(r"([%s])" % punctuations)
real_separator_regex = re.compile(r"([\.,:][^a-zA-Z0-9])")
def tokenize(s):
    s = re.sub(punctuations_regex, r" \1 ", s)
    s = re.sub(real_separator_regex, r" \1", s)
    s = s.split()
    return s
text = '"Frédéric Chopin! adalah, (1907–1986) (1907-1986) anak-anak (children): gembala. Andi\'s sheep is an \'03 R&B player; 奉獻 km² Jīdū °5 вера ʰp". Totalnya 10.000 rupiah'
# text = 'Beyoncé Giselle Knowles-Carter (/ biːˈjɒnseɪ / bee-YON-say) (lahir 4 September 1981) adalah penyanyi, penulis lagu, produser dan aktris rekaman Amerika. Dilahirkan dan dibesarkan di Houston, Texas, ia tampil di berbagai kompetisi menyanyi dan menari sebagai seorang anak, dan mulai terkenal pada akhir 1990-an sebagai penyanyi utama grup gadis R&B Destiny\'s Child. Dikelola oleh ayahnya, Mathew Knowles, kelompok ini menjadi salah satu kelompok gadis terlaris di dunia sepanjang masa. Hiatus mereka melihat rilis album debut Beyoncé, Dangerously in Love (2003), yang menetapkan dia sebagai artis solo di seluruh dunia, memperoleh lima Grammy Awards dan menampilkan Billboard Hot 100 nomor satu single "Crazy in Love" dan "Baby Boy" .'
tokenize(normalize_string(text))

"""### Padding"""

TAKEN_QUANTILE = 0.99

def pad_tensor(tensor, value, length):
    tensor.extend([value for _ in range(length - len(tensor))])

def get_sentence_and_question_max_length():
    sentence_lengths = []
    for paragraph in df_squad['paragraphs']:
        for qas in paragraph:
            context_sentences = nltk.tokenize.sent_tokenize(qas['context'])
            for sentence in context_sentences:
                sentence_lengths.append(len(tokenize(normalize_string(sentence))))
    df_sentence_lengths = pd.DataFrame(sentence_lengths)
    sentence_lengths_desc = df_sentence_lengths.describe()
    print(sentence_lengths_desc, end='\n\n')

    question_lengths = []
    for paragraph in df_squad['paragraphs']:
        for qas in paragraph:
            for qa in qas['qas']:
                question_lengths.append(len(tokenize(qa['question'])))
    df_question_lengths = pd.DataFrame(question_lengths)
    question_lengths_desc = df_question_lengths.describe()
    print(question_lengths_desc, end='\n\n')

    sentence_quantile = df_sentence_lengths.quantile(TAKEN_QUANTILE)[0].astype(int)
    question_quantile = df_question_lengths.quantile(TAKEN_QUANTILE)[0].astype(int) + 1 # +1 for EOS token
    return sentence_quantile , question_quantile

SENTENCE_MAX_LENGTH, QUESTION_MAX_LENGTH = get_sentence_and_question_max_length()
QUESTION_MAX_LENGTH += 1    # include EOS_TOKEN
print(SENTENCE_MAX_LENGTH)
print(QUESTION_MAX_LENGTH)

"""### Answer Preprocessor"""

from fuzzywuzzy import fuzz

WORD_SIMILARITY_THRESHOLD = 80

def convert_charloc_to_wordloc(tokenized_context, tokenized_words, char_loc):
    if len(tokenized_words) == 0:
        return -2

    pointer_loc = 0
    i = 0
    j = 0
    while i < len(tokenized_context) and j < min(2, len(tokenized_words)):
        if char_loc-pointer_loc <= 5:
            if tokenized_context[i].isnumeric():
                similarity = fuzz.ratio(tokenized_context[i], tokenized_words[j])
            else:
                similarity = fuzz.partial_ratio(tokenized_context[i], tokenized_words[j])
            # print(f'{tokenized_context[i]} vs {tokenized_words[j]} = {similarity}')
            if similarity >= WORD_SIMILARITY_THRESHOLD:
                j += 1
        pointer_loc += len(tokenized_context[i]) + 1
        i += 1
    if j >= min(2, len(tokenized_words)):
        return i-j
    else:
        return -1

def is_end_punctuations(token):
    return token in '.!?'

def get_sentence_location_from_answer_word_index(tokenized_context, answer_word_idx):
    start_idx = answer_word_idx-1
    end_idx = answer_word_idx
    while start_idx > -1 and not is_end_punctuations(tokenized_context[start_idx]):
        start_idx -= 1
    while end_idx < len(tokenized_context)-1 and not is_end_punctuations(tokenized_context[end_idx]):
        end_idx += 1
    return start_idx+1, end_idx

context = 'Aku adalah. Anak gembala! Selalu riang.serta gembira? Karena aku raj!in bek?erja tak pernah lengah ataupun lelah.. Lalala'
tokenized_context = tokenize(normalize_string(context))
answer_idx = 0
start_idx, end_idx = get_sentence_location_from_answer_word_index(tokenized_context, answer_idx)
print(' '.join(tokenized_context[start_idx:end_idx+1]))

"""### NER Preprocessor"""

def create_ner_tensor(tokenized_context, entities, ner_textdict):
    ner_tensor = [0 for _ in range(len(tokenized_context))]

    if len(entities) == 0:
        return ner_tensor

    pointer_loc = 0
    i = 0
    j = 0
    k = 0
    entities_name = tokenize(entities[j]['name'])
    while i < len(tokenized_context) and entities_name != None:
        pointer_loc += len(tokenized_context[i]) + 1
        if entities[j]['begin_offset']-pointer_loc <= 0:
            similarity = fuzz.partial_ratio(tokenized_context[i], entities_name[k])
            # print(f'{tokenized_context[i]} vs {entities_name[k]} = {similarity}')
            if similarity >= WORD_SIMILARITY_THRESHOLD:
                ner_tensor[i] = ner_textdict.word2index[entities[j]['type']]
                k += 1
                if k == len(entities_name):
                    j += 1
                    k = 0
                    entities_name = None if j == len(entities) else tokenize(entities[j]['name'])
            i += 1
    
    return ner_tensor

context = df_squad.iloc[0]['paragraphs'][0].get('context')
print(context)
entities = df_squad.iloc[0]['paragraphs'][0].get('entities')
print(entities)
print(create_ner_tensor(tokenize(normalize_string(context)), entities, ner_textdict))

"""### PosTags Preprocessor"""

FULL_MATCH = 100

def flatten(list):
    new_list = []
    for list_ in list:
        new_list.extend(list_)
    return new_list

def calc_n_gram_similarity(n, token_1, postags, j):
    n_gram = ''
    if j+n < len(postags):
        for k in range(n):
            n_gram += postags[j+k][0]
        # print(f'{n}-gram: {token_1} vs {n_gram} = {fuzz.ratio(token_1, n_gram)}')
        return fuzz.ratio(token_1, n_gram)
    else:
        return 0

MAX_N_GRAM = 5
def create_postags_tensor(tokenized_context, postags_, postags_textdict):
    pos_tensor = [0 for _ in range(len(tokenized_context))]

    if len(postags_) == 0:
        return pos_tensor

    average_sim = []
    j = 0
    postags = flatten(postags_)
    for i in range(len(tokenized_context)):
        n = 1
        found = False
        iter_limit = MAX_N_GRAM - max(0, i + 5 - len(tokenized_context))
        prev_n_gram_similarity = 0
        while n <= iter_limit and not found:
            n_gram_similarity = calc_n_gram_similarity(n, tokenized_context[i], postags, j)
            if n_gram_similarity == 0:
                pos_tensor[i] = NONE_NER_POS_TOKEN
                found = True
            elif n_gram_similarity != FULL_MATCH and n < iter_limit:
                if n_gram_similarity > prev_n_gram_similarity:
                    prev_n_gram_similarity = n_gram_similarity
                elif n_gram_similarity <= prev_n_gram_similarity:
                    j -= 1
                    pos_tensor[i] = postags_textdict.word2index[postags[j-n+1][1]]
                    # print(f'\t{tokenized_context[i]} {postags[j-n+1][1]}')
                    j += n
                    found = True
            elif n_gram_similarity >= WORD_SIMILARITY_THRESHOLD:
                pos_tensor[i] = postags_textdict.word2index[postags[j-n+1][1]]
                # print(f'\t{tokenized_context[i]} {postags[j-n+1][1]}')
                j += n
                found = True
            else:
                pos_tensor[i] = NONE_NER_POS_TOKEN
                found = True
            n += 1
        average_sim.append(n_gram_similarity if n_gram_similarity > prev_n_gram_similarity else prev_n_gram_similarity)
    # average_sim = sum(average_sim)/len(average_sim)
    # print(f'Average similarity: {average_sim:.2f}%')
    return pos_tensor

taken_topic_idx = 0
taken_content_idx = 10
context = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('context')
print(context)
postags = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_content_idx].get('postags')
print(postags)
print(create_postags_tensor(tokenize(normalize_string(context)), postags, postags_textdict))

"""## Action!"""

import string
import time 


input_tensors = []
is_answer_tensors = []
is_cased_tensors = []
is_number_tensors = []
ner_tensors = []
pos_tensors = []
target_tensors = []

deleted = 0

start_time = time.time()
for taken_topic_idx in range(df_squad.shape[0]):
    for taken_context_idx in range(len(df_squad.iloc[taken_topic_idx]['paragraphs'])):
        context = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['context']
        tokenized_context = tokenize(normalize_string(context))
        text_dict.addWords(tokenized_context)
        count_tobe_removed_chars = len(re.findall(non_ascii_regex, unicode_to_ascii(context))) * 1.5  # With assumption every nonascii is followed by space

        try:
            entities = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['entities']
            postags = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['postags']
        except KeyError:
            print(f'Key Error, skipped ({taken_topic_idx}, {taken_content_idx})')
            i += 1
            continue
        entities = create_ner_tensor(tokenized_context, entities, ner_textdict)
        postags = create_postags_tensor(tokenized_context, postags, postags_textdict)

        qas = df_squad.iloc[taken_topic_idx]['paragraphs'][taken_context_idx]['qas']
        i = 0
        while i < len(qas):
            qa = qas[i]

            indonesian_answer = qa.get('indonesian_answers') or qa.get('indonesian_plausible_answers')
            # indonesian_answer = qa.get('indonesian_answers')
            # if indonesian_answer is None:
            #     print(f'Found impossible question, Skipped..')
            #     i += 1
            #     continue
            tokenized_answers = tokenize(normalize_string(indonesian_answer[0]['text']))
            answer_start = indonesian_answer[0]['answer_start'] - count_tobe_removed_chars
            answer_idx = convert_charloc_to_wordloc(tokenized_context, tokenized_answers, answer_start)
            if answer_idx < 0:
                print(f'Not found at topic_idx={taken_topic_idx}, taken_context_idx={taken_context_idx}, qas_idx={i}')
                deleted += 1
                qas.pop(i)
                continue

            sent_start_idx, sent_end_idx = get_sentence_location_from_answer_word_index(tokenized_context, answer_idx)
            tokenized_sentence = tokenized_context[sent_start_idx:sent_end_idx+1]
            answer_idx -= sent_start_idx
            sentence_tensor = [text_dict.word2index[word] for word in tokenized_sentence]
            if sent_end_idx-sent_start_idx+1 > SENTENCE_MAX_LENGTH:
                # print(f'Sentence too long, skipped ({taken_topic_idx}, {taken_content_idx})')
                i += 1
                continue

            ner_tensor = [entities[i] for i in range(sent_start_idx, sent_end_idx+1)]
            pos_tensor = [postags[i] for i in range(sent_start_idx, sent_end_idx+1)]
            pad_tensor(sentence_tensor, 0, SENTENCE_MAX_LENGTH)
            pad_tensor(ner_tensor, 0, SENTENCE_MAX_LENGTH)
            pad_tensor(pos_tensor, 0, SENTENCE_MAX_LENGTH)

            is_answer_tensor = []
            is_cased_tensor = []
            is_number_tensor = []
            for j in range(SENTENCE_MAX_LENGTH):
                is_answer_tensor.append(
                    1 if j in range(answer_idx, answer_idx+len(tokenized_answers)) \
                    else 0
                )
                is_number_tensor.append(
                    1 if j<len(tokenized_sentence) and tokenized_sentence[j].isnumeric() \
                    else 0
                )
                is_cased_tensor.append(
                    1 if j<len(tokenized_sentence) and tokenized_sentence[j][0].isupper() \
                    else 0
                )

            indonesian_question = qa['question']
            tokenized_questions = tokenize(normalize_string(indonesian_question))
            if len(tokenized_questions)+1 > QUESTION_MAX_LENGTH:
                # print(f'Question too long, skipped ({taken_topic_idx}, {taken_content_idx})')
                i += 1
                continue
            text_dict.addWords(tokenized_questions)
            # if tokenized_questions[0].lower() != "kapan":
            #     # print(f'Only accept "kapan" question. Skipping..')
            #     i += 1
            #     continue
            question_tensor = [text_dict.word2index[word] for word in tokenized_questions]
            question_tensor.append(EOS_TOKEN)
            pad_tensor(question_tensor, 0, QUESTION_MAX_LENGTH)

            input_tensors.append(sentence_tensor)
            is_answer_tensors.append(is_answer_tensor)
            is_cased_tensors.append(is_cased_tensor)
            is_number_tensors.append(is_number_tensor)
            ner_tensors.append(ner_tensor)
            pos_tensors.append(pos_tensor)
            target_tensors.append(question_tensor)

            i += 1
end_time = time.time()

total_questions = total_questions - deleted
print(f'Not found answers: {deleted}. Questions left: {total_questions - deleted}')
print(f'Execution time: {end_time-start_time}')

for i in range(len(input_tensors)):
    sentence_tensor = input_tensors[i]
    is_answer_tensor = is_answer_tensors[i]
    sentence = ' '.join([text_dict.index2word[word_idx] for word_idx in sentence_tensor])
    answer = ' '.join([text_dict.index2word[sentence_tensor[idx]] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
    print(sentence)
    print(f'Answer: {answer}')
    if i > 10:
        break

def concat_feature_tensors(*args):
    n_features = len(args)
    assert n_features > 1
    concated = np.array(args[0]).reshape(len(args[0]), -1, 1)
    for i in range(n_features-1):
        concated = np.concatenate(
            (concated, np.array(args[i+1]).reshape(len(args[i+1]), -1, 1)),
            axis = -1
            )
    return concated

feature_tensors = concat_feature_tensors(is_answer_tensors, is_cased_tensors, is_number_tensors, ner_tensors, pos_tensors)

input_tensors = torch.from_numpy(np.array(input_tensors)).long().view(len(input_tensors), -1, 1).to(device)
feature_tensors = torch.tensor(feature_tensors).float().to(device)
target_tensors = torch.from_numpy(np.array(target_tensors)).long().view(len(target_tensors), -1, 1).to(device)
print(input_tensors.size())
print(feature_tensors.size())
print(target_tensors.size())

# Count rows which contain non-zero element
np.unique(np.array(is_answer_tensors).nonzero()[0]).shape

"""# Create Weights Matrix Embedding using FastText"""

OOV_REPLACEMENT = '<unk>'

print(len(text_dict.index2word))

weights_matrix = np.zeros((len(text_dict.index2word), EMBEDDING_SIZE))

def convert_word_to_word_embedding(word, model):
    try:
        word_embedding = model.wv[word]
    except KeyError as e:
        print(f'{type(e).__name__} exception: {e}.')
        word_embedding = model.wv[OOV_REPLACEMENT]
    return word_embedding

for i, word in text_dict.index2word.items():
    weights_matrix[i] = convert_word_to_word_embedding(word, model)
print(f'All unknown words will be replaced with {OOV_REPLACEMENT}')

weights_matrix = torch.from_numpy(weights_matrix)
print(weights_matrix.size())

import gc
gc.collect()

"""# Sequence to Sequence"""

def create_emb_layer(weights_matrix, non_trainable=False):
    vocab_size, embedding_dim = weights_matrix.size()
    emb_layer = nn.Embedding(vocab_size, embedding_dim)
    emb_layer.load_state_dict({'weight': weights_matrix})
    if non_trainable:
        emb_layer.weight.requires_grad = False

    return emb_layer, vocab_size, embedding_dim

"""## Encoder"""

class EncoderRNN(nn.Module):
    def __init__(self, weights_matrix, feature_tensors, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding, _, embedding_dim = create_emb_layer(weights_matrix)
        intermediate_dim = embedding_dim + feature_tensors.size(2)
        self.gru = nn.GRU(intermediate_dim, hidden_size)

    def forward(self, input, feature_tensor, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output = torch.cat((embedded, feature_tensor.view(1, 1, -1)), 2)
        output, hidden = self.gru(output, hidden)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

"""## Attention Decoder"""

class AttnDecoderRNN(nn.Module):
    def __init__(self, weights_matrix, hidden_size, output_size, dropout_p=0.1, max_length=SENTENCE_MAX_LENGTH):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding, _, embedding_dim = create_emb_layer(weights_matrix)
        self.attn = nn.Linear(embedding_dim + hidden_size, self.max_length)
        self.attn_combine = nn.Linear(embedding_dim + hidden_size, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        self.gru = nn.GRU(self.hidden_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        # embedded = self.dropout(embedded)

        attn_weights = F.softmax(
            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)

        output = F.relu(output)
        output, hidden = self.gru(output, hidden)

        output = F.log_softmax(self.out(output[0]), dim=1)
        return output, hidden, attn_weights

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

"""# Training

## Helper Function
"""

import time
import math
import random
import os


os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(42)
random.seed(42)


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return f'{m}m {s}s'


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return f'{asMinutes(s)} (- {asMinutes(rs)})'

"""## Beam Search Reference
https://github.com/budzianowski/PyTorch-Beam-Search-Decoding/blob/master/decode_beam.py

## Train
"""

teacher_forcing_ratio = 1


def train(input_tensor, feature_tensor, target_tensor, encoder, decoder, criterion, max_length=SENTENCE_MAX_LENGTH):
    # encoder_hidden = (encoder.initHidden(), encoder.initHidden())
    encoder_hidden = encoder.initHidden()

    input_timestep = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    for ei in range(input_timestep):
        encoder_output, encoder_hidden = encoder(
            input_tensor[ei], feature_tensor[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS_TOKEN]], device=device)

    decoder_hidden = encoder_hidden

    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # Teacher forcing
            if decoder_input.item() == EOS_TOKEN:
                break

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS_TOKEN:
                break

    loss.backward()

    return loss.item() / di

SAVE_PATH = '../../models/checkpoints/checkpoint_gru.tar'
N_TAKEN_DATA = input_tensors.size(0)

def create_optimizer(optimizer, parameters, lr, betas=(0.9, 0.999)):
    if optimizer == 'sgd':
        return optim.SGD(parameters, lr)
    elif optimizer == 'adam':
        return optim.Adam(parameters, lr, betas)

def trainIters(encoder, decoder, n_epochs, batch_size=1, checkpoint=None, print_every=1000, plot_every=100, save_every=-1, learning_rate=0.01, optimizer='adam'):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every
    prev_epoch = 1
    prev_iter = 0

    encoder_optimizer = create_optimizer(optimizer, encoder.parameters(), lr=learning_rate)
    decoder_optimizer = create_optimizer(optimizer, decoder.parameters(), lr=learning_rate)
    # encoder_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(encoder_optimizer, step_scheduler)
    # decoder_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(decoder_optimizer, step_scheduler)
    criterion = nn.NLLLoss()

    if checkpoint is not None:
        prev_epoch = checkpoint['epoch']
        prev_iter = checkpoint['iter']
        plot_losses = checkpoint['plot_losses']
        encoder.load_state_dict(checkpoint['encoder_state_dict'])
        encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])
        decoder.load_state_dict(checkpoint['decoder_state_dict'])
        decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])
        print_loss_total = checkpoint['loss']
        plot_loss_total = checkpoint['loss']
        # encoder_lr_scheduler.load_state_dict(checkpoint['scheduler'])
        # decoder_lr_scheduler.load_state_dict(checkpoint['scheduler'])
        encoder.train()
        decoder.train()

    n_iters = N_TAKEN_DATA

    prev_epoch = epoch_start = prev_epoch + (prev_iter + 1) // n_iters
    prev_iter  = iter_start  = (prev_iter + 1) % n_iters

    print(f'Starting from epoch-{epoch_start}')
    print(f'Starting from iteration-{iter_start}')
    for epoch in range(epoch_start, n_epochs + 1):
        for iter in range(iter_start + 1, n_iters + 1):
            input_tensor = input_tensors[iter % n_iters]
            feature_tensor = feature_tensors[iter % n_iters]
            target_tensor = target_tensors[iter % n_iters]

            if iter % batch_size == 0:
                encoder_optimizer.zero_grad()
                decoder_optimizer.zero_grad()

            loss = train(input_tensor, feature_tensor, target_tensor, encoder,
                        decoder, criterion)
            print_loss_total += loss
            plot_loss_total += loss

            # opt.param_groups[0]['lr'] = 1e-4
            # lr_scheduler.base_lrs=[1e-4]
            # print('change', lr_scheduler.state_dict())

            if iter % batch_size == batch_size-1 or iter == n_iters:
                encoder_optimizer.step()
                decoder_optimizer.step()

            # if iter == n_iters:
            #     encoder_lr_scheduler.step()
            #     decoder_lr_scheduler.step()

            if iter % print_every == 0:
                print_loss_avg = print_loss_total / print_every
                print_loss_total = 0
                progress_percent = ((epoch-1)*(n_iters)+iter - ((prev_epoch-1)*n_iters+prev_iter)) / (max(1, n_epochs-prev_epoch)*(n_iters-prev_iter))
                print('%s (%d-%d %.2f%%) %.4f' % \
                      (timeSince(start, progress_percent),
                      epoch, iter, progress_percent * 100, print_loss_avg))
                # print(f'LR: {encoder_lr_scheduler.get_lr()}')

            if iter % plot_every == 0:
                plot_loss_avg = plot_loss_total / plot_every
                plot_losses.append(plot_loss_avg)
                plot_loss_total = 0

            if (save_every != -1 and iter % save_every == 0) or iter==n_iters:
                torch.save({
                    'epoch': epoch,
                    'iter': iter,
                    'plot_losses': plot_losses,
                    'encoder_state_dict': encoder.state_dict(),
                    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),
                    'decoder_state_dict': decoder.state_dict(),
                    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),
                    'loss': loss
                    # 'scheduler': encoder_lr_scheduler.state_dict()
                }, SAVE_PATH)
        iter_start = 0
        # encoder_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(encoder_optimizer, step_scheduler)
        # decoder_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(decoder_optimizer, step_scheduler)

    return plot_losses

"""# Plotting Results"""

plt.switch_backend('agg')
import matplotlib.ticker as ticker


def showPlot(points, save=False, path='./figures.jpg'):
    plt.figure()
    fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)
    if save:
        plt.savefig(path)
    else:
        plt.show()

"""# Evaluation"""

def evaluate(encoder, decoder, input_tensor, feature_tensor, max_length=SENTENCE_MAX_LENGTH):
    with torch.no_grad():
        # input_tensor = tensorFromSentence(input_lang, sentence)
        input_length = input_tensor.size()[0]
        # encoder_hidden = (encoder.initHidden(), encoder.initHidden())
        encoder_hidden = encoder.initHidden()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden = encoder(input_tensor[ei],
                                                     feature_tensor[ei],
                                                     encoder_hidden)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_TOKEN]], device=device)  # SOS

        decoder_hidden = encoder_hidden

        decoded_words = []
        decoder_attentions = torch.zeros(max_length, max_length)

        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            decoder_attentions[di] = decoder_attention.data
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == EOS_TOKEN:
                decoded_words.append('<eos>')
                break
            else:
                decoded_words.append(text_dict.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return decoded_words, decoder_attentions[:di + 1]

def evaluateRandomly(encoder, decoder, n=10):
    for _ in range(n):
        i = random.randint(0, input_tensors.size()[0])
        input_tensor = input_tensors[i]
        feature_tensor = feature_tensors[i]
        target_tensor = target_tensors[i]
        is_answer_tensor = feature_tensor[:,0]
        input_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in input_tensor])
        output_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensor])
        answer_text = ' '.join([text_dict.index2word[input_tensor[idx].item()] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
        print('>', input_text)
        print('=', output_text)
        print('-', answer_text)
        output_words, attentions = evaluate(encoder, decoder, input_tensor, feature_tensor)
        output_sentence = ' '.join(output_words)
        print('<', output_sentence)
        print('')

output_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensors[0]])
print(output_text)

"""# Train & Evaluate"""

LOAD = False #@param {type:"boolean"}
N_EPOCHS =  1 #@param {type:"integer"}
BATCH_SIZE = 4 #@param {type:"integer"}
PRINT_EVERY = 4 #@param {type:"integer"}
PLOT_EVERY = 20 #@param {type:"integer"}
SAVE_EVERY = 100 #@param {type:"integer"}
LEARNING_RATE = 1e-3 #@param {type:"number"}
OPTIMIZER = "adam" #@param ["sgd", "adam"]
# STEP_SCHEDULER = 100 #@param {type:"integer"}
# RESET_LR_EVERY_N_FRACTION_EPOCHS = 0.5 #@param {type:"number"}
# LR_MULTIPLIER = 0.5 #@param {type:"number"}

hidden_size = 256 #@param {type:"integer"}

try:
    checkpoint = torch.load(SAVE_PATH) if LOAD else None
except Exception as e:
    print(f'Unable to load checkpoint. Starting training from scratch.')
    checkpoint = None

encoder1 = EncoderRNN(weights_matrix, feature_tensors, hidden_size).to(device)
attn_decoder1 = AttnDecoderRNN(weights_matrix, hidden_size, text_dict.n_words, dropout_p=0.1).to(device)

history = trainIters(encoder1, attn_decoder1, n_epochs=N_EPOCHS, batch_size=BATCH_SIZE, checkpoint=checkpoint, print_every=PRINT_EVERY, plot_every=PLOT_EVERY, save_every=SAVE_EVERY, learning_rate=LEARNING_RATE, optimizer=OPTIMIZER)
showPlot(history)

# !cp "/content/drive/My Drive/TA/Checkpoints/checkpoint_gru.tar" "/content/drive/My Drive/TA/Checkpoints/checkpoint_gru_50epochs_4206data.tar"
# !cp "/content/drive/My Drive/TA/Checkpoints/checkpoint_gru_15epochs_4204data.tar" "/content/drive/My Drive/TA/Checkpoints/checkpoint_gru.tar"

evaluateRandomly(encoder1, attn_decoder1)

encoder = encoder1
decoder = attn_decoder1

for i in range(0, min(100, N_TAKEN_DATA)):
    input_tensor = input_tensors[i]
    feature_tensor = feature_tensors[i]
    target_tensor = target_tensors[i]
    is_answer_tensor = feature_tensor[:,0]
    input_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in input_tensor])
    output_text = ' '.join([text_dict.index2word[word_idx.item()] for word_idx in target_tensor])
    answer_text = ' '.join([text_dict.index2word[input_tensor[idx].item()] if is_answer_tensor[idx]==1 else '' for idx in range(len(is_answer_tensor))]).strip()
    print('>', input_text)
    print('=', output_text)
    print('-', answer_text)
    output_words, attentions = evaluate(encoder, decoder, input_tensor, feature_tensor)
    output_sentence = ' '.join(output_words)
    print('<', output_sentence)
    print()
    